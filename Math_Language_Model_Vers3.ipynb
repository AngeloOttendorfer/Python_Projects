{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AngeloOttendorfer/Python_Projects/blob/master/Math_Language_Model_Vers3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddtJ1B4MAoZ5",
        "outputId": "a1113547-5e4d-4eb6-f6a9-90e42eed1274"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.25.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "Successfully installed transformers-4.36.2\n"
          ]
        }
      ],
      "source": [
        "! pip install -U accelerate\n",
        "! pip install -U transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWutmxcmfYfh"
      },
      "source": [
        "# Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "mAdZGeAPferl",
        "outputId": "e5a31264-396c-4ec3-da17-a216b18b706c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"from google.colab import drive\\ndrive.mount('/content/drive')\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "\"\"\"from google.colab import drive\n",
        "drive.mount('/content/drive')\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "GyLgBteifiSr",
        "outputId": "5630ff63-8632-4661-8982-a95eb3e602c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"#Import the libraries\\nimport zipfile\\nimport os\\n\\nzip_derivatives = zipfile.ZipFile('/content/drive/MyDrive/Trainingsdaten/derivatives/derivatives.zip', 'r') #Opens the zip file in read mode\\nzip_determinant = zipfile.ZipFile('/content/drive/MyDrive/Trainingsdaten/determinant/determinant_train_data.zip', 'r') #Opens the zip file in read mode\\nzip_find_roots = zipfile.ZipFile('/content/drive/MyDrive/Trainingsdaten/find_roots/find_roots.zip', 'r') #Opens the zip file in read mode\\nzip_integrals = zipfile.ZipFile('/content/drive/MyDrive/Trainingsdaten/integrals/integrals.zip', 'r') #Opens the zip file in read mode\\nzip_invert_function = zipfile.ZipFile('/content/drive/MyDrive/Trainingsdaten/invert_function/invert_function.zip', 'r') #Opens the zip file in read mode\\nzip_orthogonalize_vectors = zipfile.ZipFile('/content/drive/MyDrive/Trainingsdaten/orthogonalize_vectors/orthogonalize_vectors_train_data.zip', 'r') #Opens the zip file in read mode\\nzip_polygons = zipfile.ZipFile('/content/drive/MyDrive/Trainingsdaten/polygons/polygons_train_data.zip', 'r') #Opens the zip file in read mode\\nzip_triangles = zipfile.ZipFile('/content/drive/MyDrive/Trainingsdaten/triangles/triangles_train_data.zip', 'r') #Opens the zip file in read mode\\nzip_derivatives.extractall('/tmp') #Extracts the files into the /tmp folder\\nzip_determinant.extractall('/tmp') #Extracts the files into the /tmp folder\\nzip_find_roots.extractall('/tmp') #Extracts the files into the /tmp folder\\nzip_integrals.extractall('/tmp') #Extracts the files into the /tmp folder\\nzip_invert_function.extractall('/tmp') #Extracts the files into the /tmp folder\\nzip_orthogonalize_vectors.extractall('/tmp') #Extracts the files into the /tmp folder\\nzip_polygons.extractall('/tmp') #Extracts the files into the /tmp folder\\nzip_triangles.extractall('/tmp') #Extracts the files into the /tmp folder\\n\\nzip_derivatives.close()\\nzip_determinant.close()\\nzip_find_roots.close()\\nzip_integrals.close()\\nzip_invert_function.close()\\nzip_orthogonalize_vectors.close()\\nzip_polygons.close()\\nzip_triangles.close()\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "\"\"\"#Import the libraries\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_derivatives = zipfile.ZipFile('/content/drive/MyDrive/Trainingsdaten/derivatives/derivatives.zip', 'r') #Opens the zip file in read mode\n",
        "zip_determinant = zipfile.ZipFile('/content/drive/MyDrive/Trainingsdaten/determinant/determinant_train_data.zip', 'r') #Opens the zip file in read mode\n",
        "zip_find_roots = zipfile.ZipFile('/content/drive/MyDrive/Trainingsdaten/find_roots/find_roots.zip', 'r') #Opens the zip file in read mode\n",
        "zip_integrals = zipfile.ZipFile('/content/drive/MyDrive/Trainingsdaten/integrals/integrals.zip', 'r') #Opens the zip file in read mode\n",
        "zip_invert_function = zipfile.ZipFile('/content/drive/MyDrive/Trainingsdaten/invert_function/invert_function.zip', 'r') #Opens the zip file in read mode\n",
        "zip_orthogonalize_vectors = zipfile.ZipFile('/content/drive/MyDrive/Trainingsdaten/orthogonalize_vectors/orthogonalize_vectors_train_data.zip', 'r') #Opens the zip file in read mode\n",
        "zip_polygons = zipfile.ZipFile('/content/drive/MyDrive/Trainingsdaten/polygons/polygons_train_data.zip', 'r') #Opens the zip file in read mode\n",
        "zip_triangles = zipfile.ZipFile('/content/drive/MyDrive/Trainingsdaten/triangles/triangles_train_data.zip', 'r') #Opens the zip file in read mode\n",
        "zip_derivatives.extractall('/tmp') #Extracts the files into the /tmp folder\n",
        "zip_determinant.extractall('/tmp') #Extracts the files into the /tmp folder\n",
        "zip_find_roots.extractall('/tmp') #Extracts the files into the /tmp folder\n",
        "zip_integrals.extractall('/tmp') #Extracts the files into the /tmp folder\n",
        "zip_invert_function.extractall('/tmp') #Extracts the files into the /tmp folder\n",
        "zip_orthogonalize_vectors.extractall('/tmp') #Extracts the files into the /tmp folder\n",
        "zip_polygons.extractall('/tmp') #Extracts the files into the /tmp folder\n",
        "zip_triangles.extractall('/tmp') #Extracts the files into the /tmp folder\n",
        "\n",
        "zip_derivatives.close()\n",
        "zip_determinant.close()\n",
        "zip_find_roots.close()\n",
        "zip_integrals.close()\n",
        "zip_invert_function.close()\n",
        "zip_orthogonalize_vectors.close()\n",
        "zip_polygons.close()\n",
        "zip_triangles.close()\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCHclPfcq7DR"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mId7wCNhtSE4"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LTmYjikGtVRD"
      },
      "outputs": [],
      "source": [
        "def last_boxed_only(sample):\n",
        "    \"\"\"\n",
        "    Given a (q,a) sample, filter the answers so that they only contain\n",
        "    the last \\boxed{...} or \\fbox{...} element\n",
        "    \"\"\"\n",
        "    q, a = sample\n",
        "    a = last_boxed_only_string(a)\n",
        "    if a == None:\n",
        "        return None\n",
        "    return (q, a)\n",
        "\n",
        "def last_boxed_only_string(string):\n",
        "    idx = string.rfind(\"\\\\boxed\")\n",
        "    if idx < 0:\n",
        "        idx = string.rfind(\"\\\\fbox\")\n",
        "        if idx < 0:\n",
        "            return None\n",
        "\n",
        "    i = idx\n",
        "    right_brace_idx = None\n",
        "    num_left_braces_open = 0\n",
        "    while i < len(string):\n",
        "        if string[i] == \"{\":\n",
        "            num_left_braces_open += 1\n",
        "        if string[i] == \"}\":\n",
        "            num_left_braces_open -= 1\n",
        "            if num_left_braces_open == 0:\n",
        "                right_brace_idx = i\n",
        "                break\n",
        "        i += 1\n",
        "\n",
        "    if right_brace_idx == None:\n",
        "        retval = None\n",
        "    else:\n",
        "        retval = string[idx:right_brace_idx + 1]\n",
        "\n",
        "    return retval\n",
        "\n",
        "def only_until_first_boxed_from_tokens(string, tokens):\n",
        "    idx = string.find(\"\\\\boxed\")\n",
        "    if idx < 0:\n",
        "        idx = string.find(\"\\\\fbox\")\n",
        "        if idx < 0:\n",
        "            return None\n",
        "\n",
        "    cum_length = 0\n",
        "    for i, t in enumerate(tokens):\n",
        "        cum_length += len(t)\n",
        "        if cum_length >= idx:\n",
        "            break\n",
        "\n",
        "    return tokens[:i]\n",
        "\n",
        "def clean_numbers(sample):\n",
        "    if not sample:\n",
        "        return None\n",
        "    new_sample = list()\n",
        "    for s in sample:\n",
        "        new_sample.append(_clean_numbers(s))\n",
        "\n",
        "    return tuple(new_sample)\n",
        "\n",
        "def _clean_numbers(string):\n",
        "    \"\"\"\n",
        "    Clean Numbers in the given string\n",
        "\n",
        "    >>> _clean_numbers(None, \"Hello 123\")\n",
        "    'Hello 123'\n",
        "    >>> _clean_numbers(None, \"Hello 1234\")\n",
        "    'Hello 1,234'\n",
        "    >>> _clean_numbers(None, \"Hello 1234324asdasd\")\n",
        "    'Hello 1,234,324asdasd'\n",
        "    \"\"\"\n",
        "    num_prev_digits = 0\n",
        "    new_string = \"\"\n",
        "    for i, c in enumerate(string):\n",
        "        # isdigit() doesnt work here because of weird unicode chars.\n",
        "        if c in {'1', '2', '3', '4', '5', '6', '7', '8', '9', '0'}:\n",
        "            num_prev_digits += 1\n",
        "        else:\n",
        "            if num_prev_digits > 3:\n",
        "                # Some fixing\n",
        "                string_number = new_string[-num_prev_digits:]\n",
        "                new_string = new_string[:-num_prev_digits] + \"{0:,}\".format(int(string_number))\n",
        "            num_prev_digits = 0\n",
        "        new_string += c\n",
        "\n",
        "    if num_prev_digits > 3:\n",
        "        # Some fixing\n",
        "        string_number = new_string[-num_prev_digits:]\n",
        "        new_string = new_string[:-num_prev_digits] + \"{0:,}\".format(int(string_number))\n",
        "\n",
        "    return new_string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqIDQyuPrBAv"
      },
      "source": [
        "## Base Math Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSJ_Um81rMij"
      },
      "source": [
        "### import necessary modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDw0-PpVrRQn",
        "outputId": "d03c2049-c2e8-4fd9-cb8d-a1d18466235f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch.nn.functional (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch.nn.functional\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement random (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for random\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement os (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for os\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement time (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for time\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install torch.nn.functional\n",
        "!pip install random\n",
        "!pip install os\n",
        "!pip install time\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yj-w5M_uUxH"
      },
      "source": [
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OGv0I9SLq_CP"
      },
      "outputs": [],
      "source": [
        "class BaseMathDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Configurable AMPS Dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataroot, tokenizer, max_tokens, mode, mode_answer='default', len_multiplier=1.0, packing=None,\n",
        "                 randomize=None, pack_end=None, clean_numbers=True, latex_mask=False, peek_fraction=(0.1, 1.0)):\n",
        "        self.dataroot = dataroot\n",
        "        self.tokenizer = tokenizer  # Set in run_training(), not in dataset creation\n",
        "        self.max_tokens = max_tokens\n",
        "        self.mode = mode\n",
        "        self.mode_answer = mode_answer  # Used in subclass\n",
        "        self.len_multiplier = len_multiplier\n",
        "        self.clean_numbers = clean_numbers\n",
        "        self.latex_mask = latex_mask\n",
        "        self.peek_fraction = peek_fraction\n",
        "\n",
        "        if self.mode in {'gpt2'}:\n",
        "            self.clean_sample = self.clean_filter_sample_gpt\n",
        "            self.packing = True\n",
        "            self.randomize = True\n",
        "            self.include_fnames = True\n",
        "            self.pack_end = True\n",
        "        elif self.mode in {'gpt2-eval'}:\n",
        "            self.clean_sample = self.clean_filter_sample_gpt_eval\n",
        "            self.packing = True\n",
        "            self.randomize = False\n",
        "            self.include_fnames = True\n",
        "            self.pack_end = True\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "        if packing != None:\n",
        "            print(\"Overriding packing to be\", packing)\n",
        "            self.packing = packing\n",
        "        if randomize != None:\n",
        "            print(\"Overriding randomize to be\", randomize)\n",
        "            self.randomize = randomize\n",
        "        if pack_end != None:\n",
        "            print(\"Overriding pack_end to be\", pack_end)\n",
        "            self.pack_end = pack_end\n",
        "\n",
        "        self.initialize()\n",
        "\n",
        "        self.bad_fnames = set()\n",
        "        self.i = 0\n",
        "\n",
        "    def initialize(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def __len__(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # Each worker needs a different seed....\n",
        "        random.seed(os.getpid() + time.time() + random.random())\n",
        "\n",
        "        # Sampling with replacement.\n",
        "        # We need to pack random elements to get close to self.max_tokens\n",
        "        curr_input_ids = []\n",
        "        curr_label_ids = []\n",
        "        curr_fnames = []\n",
        "        num_samples = 0\n",
        "        while len(curr_input_ids) + 1 <= self.max_tokens and len(curr_label_ids) + 1 <= self.max_tokens:\n",
        "            # print(\"curr_input_ids: \" + str(curr_input_ids))\n",
        "            # print(\"curr_label_ids: \" + str(curr_label_ids))\n",
        "            # print(\"curr_fnames: \" + str(curr_fnames))\n",
        "            curr_sample, fname = self.get_random_sample()\n",
        "            # print(\"current_sample: \" + str(curr_sample))\n",
        "            # print(fname)\n",
        "            if curr_sample is None:\n",
        "                # This only happens in eval modes\n",
        "                return {\n",
        "                    \"input_ids\": torch.zeros([self.max_tokens]),\n",
        "                    \"labels\": torch.zeros([self.max_tokens]),\n",
        "                    \"fnames\": [fname]\n",
        "                }\n",
        "\n",
        "            if not self.pack_end and (\n",
        "                    (len(curr_input_ids) + 1 + len(curr_sample['input_ids_list']) > self.max_tokens) or\n",
        "                    (len(curr_label_ids) + 1 + len(curr_sample['label_ids_list']) > self.max_tokens)\n",
        "            ):\n",
        "                # Do not include curr_sample if either the input_ids or the label_ids will run off the end.\n",
        "                break\n",
        "\n",
        "            # Add curr_sample to the current inputs and labels\n",
        "            # print(\"input_ids_list: \" + str(curr_sample['input_ids_list']))\n",
        "            curr_input_ids.extend(curr_sample['input_ids_list'])\n",
        "            curr_label_ids.extend(curr_sample['label_ids_list'])\n",
        "            curr_fnames.append(fname)\n",
        "\n",
        "            num_samples += 1\n",
        "\n",
        "            # Break on the first iteration if we don't want to do packing.\n",
        "            if not self.packing:\n",
        "                break\n",
        "\n",
        "        input_ids = torch.LongTensor(curr_input_ids)\n",
        "        label_ids = torch.LongTensor(curr_label_ids)\n",
        "\n",
        "        # Sanity check\n",
        "        if 'eval' not in self.mode:\n",
        "            assert len(curr_input_ids) == len(curr_label_ids)\n",
        "\n",
        "        input_ids = input_ids[:self.max_tokens]\n",
        "        label_ids = label_ids[:self.max_tokens]\n",
        "\n",
        "        if len(curr_input_ids) < self.max_tokens and 'eval' not in self.mode:\n",
        "            # Pad\n",
        "            num_to_pad = self.max_tokens - len(curr_input_ids)\n",
        "            input_ids = F.pad(input_ids, [0, num_to_pad], mode='constant', value=self.tokenizer.pad_token_id)\n",
        "\n",
        "        if len(curr_label_ids) < self.max_tokens and 'eval' not in self.mode:\n",
        "            num_to_pad = self.max_tokens - len(curr_label_ids)\n",
        "            label_ids = F.pad(label_ids, [0, num_to_pad], mode='constant', value=-100)\n",
        "\n",
        "        # Sanity check\n",
        "        if 'eval' not in self.mode:\n",
        "            assert input_ids.shape[0] == label_ids.shape[\n",
        "                0] == self.max_tokens, f\"{input_ids.shape[0]}, {label_ids.shape[0]}, {self.max_tokens}\"\n",
        "\n",
        "        if self.include_fnames:\n",
        "            return {\n",
        "                \"input_ids\": input_ids,\n",
        "                \"labels\": label_ids,\n",
        "                \"fnames\": curr_fnames\n",
        "            }\n",
        "        else:\n",
        "            # This is the format required by our GPT2Trainer class\n",
        "            return {\n",
        "                \"input_ids\": input_ids,\n",
        "                \"labels\": label_ids\n",
        "            }\n",
        "\n",
        "    def get_random_sample(self):\n",
        "        \"\"\"\n",
        "        Get a full on random sample (used for training)\n",
        "        \"\"\"\n",
        "        random_sample = None\n",
        "        while random_sample is None:\n",
        "            if self.randomize:\n",
        "                q, a, fname = random.choice(self.samples)\n",
        "            else:\n",
        "                q, a, fname = self.samples[self.i]\n",
        "                self.i = (self.i + 1) % len(self.samples)\n",
        "\n",
        "            random_sample = self.clean_sample((q, a))  # q + '\\n' + a  # self.clean_sample((q, a))\n",
        "            # print(\"random_sample: \" + str(random_sample))\n",
        "\n",
        "            if not self.randomize:\n",
        "                break\n",
        "\n",
        "        return random_sample, fname\n",
        "\n",
        "    def clean_filter_sample_gpt(self, sample):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def clean_filter_sample_gpt_eval(self, sample):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def clean_filter_sample_t5(self, sample):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def clean_filter_sample_t5_eval(self, sample):\n",
        "        raise NotImplementedError()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEcedn3BspKy"
      },
      "source": [
        "## Configurable Mathematica Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5EIu8e3uhrL"
      },
      "source": [
        "### Import necessary modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_E6t2MQfupVA",
        "outputId": "16dce010-2e8c-4217-eae3-715d39764c02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement os (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for os\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install tqdm\n",
        "!pip install os\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGV7vynivyFs"
      },
      "source": [
        " ### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WVr1T27xtDPJ"
      },
      "outputs": [],
      "source": [
        "class MathematicaMathDataset(BaseMathDataset):\n",
        "    \"\"\"Configurable Math Dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(len(self.samples))\n",
        "\n",
        "    def initialize(self):\n",
        "        \"\"\"\n",
        "        Set up self.samples by loading from the dataroot\n",
        "        \"\"\"\n",
        "\n",
        "        # with open(self.dataroot, 'r') as fp:\n",
        "            # all_filenames = fp.readlines()\n",
        "\n",
        "\n",
        "        all_filenames = os.listdir(self.dataroot)\n",
        "        if '.DS_Store' in all_filenames:\n",
        "          all_filenames.remove('.DS_Store')\n",
        "        print(\"Dataroot: \" + self.dataroot)\n",
        "        print(\"all_filenames: \" + str(all_filenames))\n",
        "\n",
        "\n",
        "        print(f\"{self.__class__.__name__}: Loading samples from {len(all_filenames)} files.\")\n",
        "        samples_raw = []\n",
        "        for fname in tqdm(all_filenames):\n",
        "            fname = os.path.join(self.dataroot, fname)\n",
        "            fname = fname.rstrip()\n",
        "            print(\"fname: \" + str(fname))\n",
        "            # print(fname)\n",
        "            # fname = os.path.join(os.path.dirname(os.path.dirname(self.dataroot)), fname[2:])\n",
        "            # print(fname)\n",
        "\n",
        "            if not os.path.isfile(fname):\n",
        "                print(f\"SKIPPING {fname}\")\n",
        "                continue\n",
        "            with open(fname, 'r') as fp:\n",
        "                question = \"\"\n",
        "                answers  = []\n",
        "                reading_question = True\n",
        "                curr_section = \"\"\n",
        "                for line in fp:\n",
        "                    if line == \"Problem:\\n\":\n",
        "                        reading_question = True\n",
        "                    elif line == \"Answer:\\n\":\n",
        "                        if reading_question:\n",
        "                            # curr_section contains Q\n",
        "                            question = curr_section\n",
        "                        else:\n",
        "                            # curr_section contains an A\n",
        "                            answers.append(curr_section)\n",
        "                        curr_section = \"\"\n",
        "                        reading_question = False\n",
        "                    else:\n",
        "                        curr_section += line\n",
        "\n",
        "                # The last answer needs to be recorded.\n",
        "                answers.append(curr_section)\n",
        "\n",
        "            for a in answers:\n",
        "                samples_raw.append((question, a, fname))\n",
        "\n",
        "        # manager = Manager()\n",
        "        # samples_raw = manager.list(samples_raw)\n",
        "        self.samples = samples_raw\n",
        "        del samples_raw\n",
        "\n",
        "        print(f\"{self.__class__.__name__}: Loaded {len(self.samples)} samples.\")\n",
        "        # print(self.samples)\n",
        "\n",
        "    def clean_filter_sample_gpt(self, sample):\n",
        "        \"\"\"\n",
        "        Does the actual tokenization. Should be parallelized because it can be a bit slow.\n",
        "        \"\"\"\n",
        "\n",
        "        if sample == None:\n",
        "            return None\n",
        "\n",
        "        question, answer = sample\n",
        "        if self.clean_numbers:\n",
        "            question = _clean_numbers(question)\n",
        "            answer = _clean_numbers(answer)\n",
        "\n",
        "        if self.mode_answer == 'default':\n",
        "            question_ids     = torch.LongTensor(self.tokenizer.encode(\"\\nQUESTION:\\n\" + question, verbose=False))\n",
        "\n",
        "            sep_ids          = torch.LongTensor(self.tokenizer.encode(\"\\nFINAL ANSWER:\\n\", verbose=False))\n",
        "            answer_ids       = self.tokenizer.encode(answer, verbose=False)\n",
        "            answer_ids.append(self.tokenizer.eos_token_id)\n",
        "            answer_ids       = torch.LongTensor(answer_ids)\n",
        "\n",
        "            # Use full solution\n",
        "            input_ids = torch.cat([\n",
        "                question_ids,\n",
        "                sep_ids,\n",
        "                answer_ids\n",
        "            ], dim=0)\n",
        "\n",
        "            label_ids = torch.cat([\n",
        "                torch.ones_like(question_ids) * -100,\n",
        "                torch.ones_like(sep_ids) * -100,\n",
        "                answer_ids.clone()\n",
        "            ], dim=0)\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "        # Stop early if this Q,A pair is too long\n",
        "        if input_ids.shape[0] > self.max_tokens:\n",
        "            # Print reason for skipping\n",
        "            # print(f\"{self.__class__.__name__} Skipping due to input_ids being too big. input_ids.shape[0] = {input_ids.shape[0]}.\")\n",
        "            return None\n",
        "\n",
        "        input_ids = input_ids.tolist()\n",
        "        label_ids = label_ids.tolist()\n",
        "\n",
        "        return {\n",
        "            'input_ids_list' : input_ids,\n",
        "            'label_ids_list' : label_ids\n",
        "        }\n",
        "\n",
        "    def clean_filter_sample_t5(self, sample):\n",
        "        \"\"\"\n",
        "        Does the actual tokenization. Should be parallelized because it can be a bit slow.\n",
        "        \"\"\"\n",
        "\n",
        "        if sample == None:\n",
        "            return None\n",
        "\n",
        "        question, answer = sample\n",
        "        if self.clean_numbers:\n",
        "            question = _clean_numbers(question)\n",
        "            answer = _clean_numbers(answer)\n",
        "\n",
        "        if self.mode_answer == 'default':\n",
        "            question_ids     = torch.LongTensor(self.tokenizer.encode(\"\\nQUESTION:\\n\" + question + \"\\nFINAL ANSWER:\\n\", verbose=False))\n",
        "            answer_ids       = torch.LongTensor(self.tokenizer.encode(answer, verbose=False))\n",
        "\n",
        "            input_ids = torch.cat([\n",
        "                question_ids,\n",
        "            ], dim=0)\n",
        "\n",
        "            label_ids = torch.cat([\n",
        "                answer_ids\n",
        "            ], dim=0)\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "        # Stop early if this Q,A pair is too long\n",
        "        if input_ids.shape[0] > self.max_tokens:\n",
        "            # Print reason for skipping\n",
        "            # print(f\"{self.__class__.__name__} Skipping due to input_ids being too big. input_ids.shape[0] = {input_ids.shape[0]}.\")\n",
        "            return None\n",
        "\n",
        "        input_ids = input_ids.tolist()\n",
        "        label_ids = label_ids.tolist()\n",
        "\n",
        "        return {\n",
        "            'input_ids_list' : input_ids,\n",
        "            'label_ids_list' : label_ids\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCtvq5gSsnES"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzO0KdTvwMSw"
      },
      "source": [
        "## MATH Dataset Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nhBVNWhwTMG"
      },
      "source": [
        "### import necessary modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cndWlUKEwW5o",
        "outputId": "3d70d6ad-b582-49de-9eb2-58cfe7665ece"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement json (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for json\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Invalid requirement: 'glob!pip'\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install json\n",
        "!pip install glob!pip install random\n",
        "!pip install numpy\n",
        "\n",
        "import torch\n",
        "import json\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from multiprocessing import Manager"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UqmXtDOxI_2"
      },
      "source": [
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Rqm95sg6xLUS"
      },
      "outputs": [],
      "source": [
        "class MATHDataset(BaseMathDataset):\n",
        "    \"\"\"Configurable Math Dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(len(self.samples) * self.len_multiplier)\n",
        "\n",
        "    def initialize(self):\n",
        "        \"\"\"\n",
        "        Set up self.samples by loading from the dataroot\n",
        "        \"\"\"\n",
        "        print(self.dataroot)\n",
        "        all_filenames = glob.glob('/Users/angeloottendorfer/Desktop/amps/mathematica/algebra/testdaten_find_roots/*')\n",
        "        print(all_filenames)\n",
        "        samples_raw = []\n",
        "        for fname in all_filenames:\n",
        "            with open(fname, 'r') as fp:\n",
        "                try:\n",
        "                    problem_data = json.load(fp)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading JSON from {fname}\", e)\n",
        "                    raise e\n",
        "            curr_sample_raw = (problem_data['problem'], problem_data['solution'], fname)\n",
        "            for e in curr_sample_raw:\n",
        "                assert e\n",
        "            samples_raw.append(curr_sample_raw)\n",
        "\n",
        "        manager = Manager()\n",
        "        samples_raw = manager.list(samples_raw)\n",
        "        self.samples = samples_raw\n",
        "        del samples_raw\n",
        "\n",
        "        print(f\"{self.__class__.__name__}: Loaded {len(self.samples)} samples.\")\n",
        "\n",
        "    def clean_filter_sample_gpt(self, sample):\n",
        "        \"\"\"\n",
        "        Does the actual tokenization. Should be parallelized because it can be a bit slow.\n",
        "        \"\"\"\n",
        "\n",
        "        if sample == None:\n",
        "            return None\n",
        "\n",
        "        if self.mode_answer == 'peeking_only':\n",
        "            return self.clean_filter_sample_peeking_gpt(sample)\n",
        "        if self.mode_answer == 'mixed_full_and_peeking':\n",
        "            if random.random() < 0.5:\n",
        "                return self.clean_filter_sample_peeking_gpt(sample)\n",
        "            else:\n",
        "                _mode_answer = 'full'\n",
        "        elif self.mode_answer == 'mixed_full_and_nopack_padding':\n",
        "            if random.random() < 0.5:\n",
        "                return self.clean_filter_sample_nopackpadding_gpt(sample)\n",
        "            else:\n",
        "                _mode_answer = 'full'\n",
        "        elif self.mode_answer == 'mixed_final_boxed_and_full':\n",
        "            if random.random() < 0.5:\n",
        "                _mode_answer = 'full'\n",
        "            else:\n",
        "                _mode_answer = 'final_boxed'\n",
        "        elif self.mode_answer == 'full':\n",
        "            _mode_answer = 'full'\n",
        "        elif self.mode_answer == 'final_boxed':\n",
        "            _mode_answer = 'final_boxed'\n",
        "        else:\n",
        "            raise NotImplementedError(f\"self.mode_answer = {self.mode_answer} not recognized.\")\n",
        "\n",
        "        if _mode_answer == 'full':\n",
        "            question, answer = sample\n",
        "\n",
        "            if self.clean_numbers:\n",
        "                question = _clean_numbers(question)\n",
        "                answer = _clean_numbers(answer)\n",
        "\n",
        "            answer_final = last_boxed_only_string(answer)\n",
        "\n",
        "            question_ids = torch.LongTensor(self.tokenizer.encode(\"\\nQUESTION:\\n\" + question, verbose=False))\n",
        "\n",
        "            sep_ids_2 = torch.LongTensor(self.tokenizer.encode(\"\\nFULL SOLUTION:\\n\", verbose=False))\n",
        "            answer_ids = self.tokenizer.encode(answer, verbose=False)\n",
        "            answer_ids.append(self.tokenizer.eos_token_id)\n",
        "            answer_ids = torch.LongTensor(answer_ids)\n",
        "\n",
        "            input_ids = torch.cat([\n",
        "                question_ids,\n",
        "                sep_ids_2,\n",
        "                answer_ids\n",
        "            ], dim=0)\n",
        "\n",
        "            # Only answer_ids contribute to the loss\n",
        "            label_ids = torch.cat([\n",
        "                torch.ones_like(question_ids) * -100,\n",
        "                torch.ones_like(sep_ids_2) * -100,\n",
        "                answer_ids.clone()\n",
        "            ], dim=0)\n",
        "\n",
        "        elif _mode_answer == 'final_boxed':\n",
        "            question, answer = sample\n",
        "\n",
        "            if self.clean_numbers:\n",
        "                question = _clean_numbers(question)\n",
        "                answer = _clean_numbers(answer)\n",
        "            answer_final = last_boxed_only_string(answer)\n",
        "            if not answer_final:\n",
        "                print(\"ERROR FROM\", question, answer)\n",
        "                return None\n",
        "\n",
        "            question_ids = torch.LongTensor(self.tokenizer.encode(\"\\nQUESTION:\\n\" + question, verbose=False))\n",
        "\n",
        "            sep_ids_1 = torch.LongTensor(self.tokenizer.encode(\"\\nFINAL ANSWER:\\n\", verbose=False))\n",
        "            answer_final_ids = self.tokenizer.encode(answer_final, verbose=False)\n",
        "            answer_final_ids.append(self.tokenizer.eos_token_id)\n",
        "            answer_final_ids = torch.LongTensor(answer_final_ids)\n",
        "\n",
        "            input_ids = torch.cat([\n",
        "                question_ids,\n",
        "                sep_ids_1,\n",
        "                answer_final_ids,\n",
        "            ], dim=0)\n",
        "\n",
        "            # Only answer_ids contribute to the loss\n",
        "            label_ids = torch.cat([\n",
        "                torch.ones_like(question_ids) * -100,\n",
        "                torch.ones_like(sep_ids_1) * -100,\n",
        "                answer_final_ids.clone(),\n",
        "            ], dim=0)\n",
        "\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "        # Stop early if this Q,A pair is too long\n",
        "        if input_ids.shape[0] > self.max_tokens:\n",
        "            # Print reason for skipping\n",
        "            # print(f\"Skipping due to input_ids being too big. input_ids.shape[0] = {input_ids.shape[0]}.\")\n",
        "            return None\n",
        "\n",
        "        input_ids = input_ids.tolist()\n",
        "        label_ids = label_ids.tolist()\n",
        "\n",
        "        return {\n",
        "            'input_ids_list': input_ids,\n",
        "            'label_ids_list': label_ids\n",
        "        }\n",
        "\n",
        "    def clean_filter_sample_nopackpadding_gpt(self, sample):\n",
        "\n",
        "        if sample == None:\n",
        "            return None\n",
        "\n",
        "        question, answer = sample\n",
        "\n",
        "        if self.clean_numbers:\n",
        "            question = _clean_numbers(question)\n",
        "            answer = _clean_numbers(answer)\n",
        "\n",
        "        answer_final = last_boxed_only_string(answer)\n",
        "\n",
        "        question_ids = torch.LongTensor(self.tokenizer.encode(\"\\nQUESTION:\\n\" + question, verbose=False))\n",
        "        sep_ids = torch.LongTensor(self.tokenizer.encode(\"\\nFINAL ANSWER:\\n\", verbose=False))\n",
        "        final_answer_ids = torch.LongTensor(self.tokenizer.encode(answer_final, verbose=False))\n",
        "\n",
        "        # Stop early if this Q,A pair is too long\n",
        "        num_to_pad = 32\n",
        "        padding_tensor = torch.ones((num_to_pad)) * 220  # 220 is the token for space in the case of GPT2 models\n",
        "\n",
        "        input_ids = torch.cat([\n",
        "            question_ids,\n",
        "            padding_tensor,\n",
        "            sep_ids,\n",
        "            final_answer_ids\n",
        "        ], dim=0)\n",
        "\n",
        "        # Only answer_ids contribute to the loss\n",
        "        label_ids = torch.cat([\n",
        "            torch.ones_like(question_ids) * -100,\n",
        "            torch.ones_like(padding_tensor) * -100,\n",
        "            torch.ones_like(sep_ids) * -100,\n",
        "            final_answer_ids.clone()\n",
        "        ], dim=0)\n",
        "\n",
        "        input_ids = input_ids.tolist()\n",
        "        label_ids = label_ids.tolist()\n",
        "\n",
        "        return {\n",
        "            'input_ids_list': input_ids,\n",
        "            'label_ids_list': label_ids\n",
        "        }\n",
        "\n",
        "    def clean_filter_sample_nopackpadding_gpt_eval(self, sample):\n",
        "\n",
        "        if sample == None:\n",
        "            return None\n",
        "\n",
        "        question, answer = sample\n",
        "\n",
        "        if self.clean_numbers:\n",
        "            question = _clean_numbers(question)\n",
        "            answer = _clean_numbers(answer)\n",
        "\n",
        "        answer_final = last_boxed_only_string(answer)\n",
        "\n",
        "        question_ids = torch.LongTensor(self.tokenizer.encode(\"\\nQUESTION:\\n\" + question, verbose=False))\n",
        "        sep_ids = torch.LongTensor(self.tokenizer.encode(\"\\nFINAL ANSWER:\\n\", verbose=False))\n",
        "        final_answer_ids = torch.LongTensor(self.tokenizer.encode(answer_final, verbose=False))\n",
        "\n",
        "        num_to_pad = 32\n",
        "        padding_tensor = torch.ones((num_to_pad)) * 220  # 220 is the token for space in the case of GPT2 models\n",
        "\n",
        "        input_ids = torch.cat([\n",
        "            question_ids,\n",
        "            padding_tensor,\n",
        "            sep_ids,\n",
        "        ], dim=0)\n",
        "\n",
        "        # Only answer_ids contribute to the loss\n",
        "        label_ids = torch.cat([\n",
        "            final_answer_ids.clone()\n",
        "        ], dim=0)\n",
        "\n",
        "        # Stop early if this Q,A pair is too long\n",
        "        if input_ids.shape[0] + label_ids.shape[0] > self.max_tokens:\n",
        "            # Print reason for skipping\n",
        "            # print(f\"Skipping due to input_ids being too big. input_ids.shape[0] = {input_ids.shape[0]}.\")\n",
        "            return None\n",
        "\n",
        "        input_ids = input_ids.tolist()\n",
        "        label_ids = label_ids.tolist()\n",
        "\n",
        "        return {\n",
        "            'input_ids_list': input_ids,\n",
        "            'label_ids_list': label_ids\n",
        "        }\n",
        "\n",
        "    def clean_filter_sample_peeking_gpt(self, sample):\n",
        "        \"\"\"\n",
        "        Does the actual tokenization. Should be parallelized because it can be a bit slow.\n",
        "        \"\"\"\n",
        "\n",
        "        if sample == None:\n",
        "            return None\n",
        "\n",
        "        question, answer = sample\n",
        "\n",
        "        if self.clean_numbers:\n",
        "            question = _clean_numbers(question)\n",
        "            answer = _clean_numbers(answer)\n",
        "\n",
        "        answer_final = last_boxed_only_string(answer)\n",
        "\n",
        "        question_ids = torch.LongTensor(\n",
        "            self.tokenizer.encode(\"\\nQUESTION:\\n\" + question + \"\\nFULL SOLUTION:\\n\", verbose=False))\n",
        "        answer_ids = self.tokenizer.tokenize(answer)\n",
        "        answer_ids = only_until_first_boxed_from_tokens(answer, answer_ids)\n",
        "        answer_ids = torch.LongTensor(self.tokenizer.encode(answer_ids, verbose=False))\n",
        "\n",
        "        # Take a fraction\n",
        "        if isinstance(self.peek_fraction, tuple):\n",
        "            final_idx = int(len(answer_ids) * random.uniform(*self.peek_fraction))\n",
        "        else:\n",
        "            final_idx = int(len(answer_ids) * self.peek_fraction)\n",
        "\n",
        "        # # Override peeking fraction\n",
        "        # final_idx = int(len(answer_ids) * np.random.choice([0.25, 0.5, 0.75, 1.0], p=[1/6, 1/6, 1/3, 1/3]))\n",
        "\n",
        "        answer_ids = answer_ids[:final_idx]\n",
        "\n",
        "        sep_ids = torch.LongTensor(self.tokenizer.encode(\"\\nFINAL ANSWER:\\n\", verbose=False))\n",
        "        final_answer_ids = torch.LongTensor(self.tokenizer.encode(answer_ids[final_idx:]))\n",
        "\n",
        "        input_ids = torch.cat([\n",
        "            question_ids,\n",
        "            answer_ids,\n",
        "            sep_ids,\n",
        "            final_answer_ids\n",
        "        ], dim=0)\n",
        "\n",
        "        # Only answer_ids contribute to the loss\n",
        "        label_ids = torch.cat([\n",
        "            torch.ones_like(question_ids) * -100,\n",
        "            torch.ones_like(answer_ids) * -100,\n",
        "            torch.ones_like(sep_ids) * -100,\n",
        "            final_answer_ids.clone()\n",
        "        ], dim=0)\n",
        "\n",
        "        # Stop early if this Q,A pair is too long\n",
        "        if input_ids.shape[0] > self.max_tokens:\n",
        "            # Print reason for skipping\n",
        "            # print(f\"Skipping due to input_ids being too big. input_ids.shape[0] = {input_ids.shape[0]}.\")\n",
        "            return None\n",
        "\n",
        "        input_ids = input_ids.tolist()\n",
        "        label_ids = label_ids.tolist()\n",
        "\n",
        "        return {\n",
        "            'input_ids_list': input_ids,\n",
        "            'label_ids_list': label_ids\n",
        "        }\n",
        "\n",
        "    def clean_filter_sample_peeking_gpt_eval(self, sample):\n",
        "        \"\"\"\n",
        "        Does the actual tokenization. Should be parallelized because it can be a bit slow.\n",
        "        \"\"\"\n",
        "\n",
        "        if sample == None:\n",
        "            return None\n",
        "\n",
        "        question, answer = sample\n",
        "\n",
        "        if self.clean_numbers:\n",
        "            question = _clean_numbers(question)\n",
        "            answer = _clean_numbers(answer)\n",
        "\n",
        "        answer_final = last_boxed_only_string(answer)\n",
        "\n",
        "        question_ids = torch.LongTensor(\n",
        "            self.tokenizer.encode(\"\\nQUESTION:\\n\" + question + \"\\nFULL SOLUTION:\\n\", verbose=False))\n",
        "        answer_ids = self.tokenizer.tokenize(answer)\n",
        "        answer_ids_full = torch.LongTensor(self.tokenizer.encode(answer))\n",
        "        answer_ids = only_until_first_boxed_from_tokens(answer, answer_ids)\n",
        "        if len(answer_ids) == 0:\n",
        "            return None\n",
        "        answer_ids = torch.LongTensor(self.tokenizer.encode(answer_ids, verbose=False))\n",
        "\n",
        "        # Take a fraction\n",
        "        if isinstance(self.peek_fraction, tuple):\n",
        "            final_idx = int(len(answer_ids) * random.uniform(*self.peek_fraction))\n",
        "        else:\n",
        "            final_idx = int(len(answer_ids) * self.peek_fraction)\n",
        "\n",
        "        answer_ids = answer_ids[:final_idx]\n",
        "\n",
        "        # sep_ids          = torch.LongTensor(self.tokenizer.encode(\"\\nFINAL ANSWER\\n\", verbose=False))\n",
        "        final_answer_ids = answer_ids_full[final_idx:]\n",
        "        print(final_answer_ids)\n",
        "\n",
        "        input_ids = torch.cat([\n",
        "            question_ids,\n",
        "            answer_ids,\n",
        "            # sep_ids,\n",
        "        ], dim=0)\n",
        "\n",
        "        # Only answer_ids contribute to the loss\n",
        "        label_ids = torch.cat([\n",
        "            final_answer_ids.clone()\n",
        "        ], dim=0)\n",
        "\n",
        "        # Stop early if this Q,A pair is too long\n",
        "        if input_ids.shape[0] + label_ids.shape[0] > self.max_tokens:\n",
        "            # Print reason for skipping\n",
        "            # print(f\"Skipping due to input_ids being too big. input_ids.shape[0] = {input_ids.shape[0]}.\")\n",
        "            return None\n",
        "\n",
        "        input_ids = input_ids.tolist()\n",
        "        label_ids = label_ids.tolist()\n",
        "\n",
        "        return {\n",
        "            'input_ids_list': input_ids,\n",
        "            'label_ids_list': label_ids\n",
        "        }\n",
        "\n",
        "    def clean_filter_sample_gpt_eval(self, sample):\n",
        "        \"\"\"\n",
        "        Does tokenization for final model evaluation. This should return\n",
        "        input_ids as the context and labels as the true answer.\n",
        "        \"\"\"\n",
        "\n",
        "        if sample == None:\n",
        "            return None\n",
        "\n",
        "        if self.mode_answer == 'eval_peeking':\n",
        "            return self.clean_filter_sample_peeking_gpt_eval(sample)\n",
        "        elif self.mode_answer == 'eval_nopack_padding':\n",
        "            return self.clean_filter_sample_nopackpadding_gpt_eval(sample)\n",
        "\n",
        "        question, answer = sample\n",
        "        print(\"question_from_sample: \" + question)\n",
        "        print(\"answer from sample: \" + answer)\n",
        "\n",
        "        if self.clean_numbers:\n",
        "            question = _clean_numbers(question)\n",
        "            answer = _clean_numbers(answer)\n",
        "            print(\"question: \" + question)\n",
        "            print(\"answer: \" + answer)\n",
        "        # answer_final = last_boxed_only_string(answer)\n",
        "        # print(\"answer_final: \" + str(answer_final))\n",
        "\n",
        "        assert not answer.isspace()\n",
        "\n",
        "        question_ids = torch.LongTensor(self.tokenizer.encode(\"\\nQUESTION:\\n\" + question, verbose=False))\n",
        "        sep_ids = torch.LongTensor(self.tokenizer.encode(\"\\FULL SOLUTION:\\n\", verbose=False))\n",
        "        answer_final_ids = torch.LongTensor(\n",
        "            self.tokenizer.encode(answer, verbose=False))  # Loss only counted on these tokens.\n",
        "\n",
        "        input_ids = torch.cat([\n",
        "            question_ids,\n",
        "            sep_ids,\n",
        "        ], dim=0)\n",
        "\n",
        "        label_ids = torch.cat([\n",
        "            answer_final_ids.clone()\n",
        "        ], dim=0)\n",
        "\n",
        "        # Stop early if this Q,A pair is too long\n",
        "        if input_ids.shape[0] + label_ids.shape[0] > self.max_tokens:\n",
        "            # Print reason for skipping\n",
        "            # print(f\"Skipping due to input_ids being too big. input_ids.shape[0] = {input_ids.shape[0]}.\")\n",
        "            return None\n",
        "\n",
        "        return {\n",
        "            'input_ids_list': input_ids.tolist(),\n",
        "            'label_ids_list': label_ids.tolist()\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTqUcVxZy07S"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4etm7OjzEXW"
      },
      "source": [
        "## fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr5hvI8OzKm1"
      },
      "source": [
        "### import necessary modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        },
        "id": "L5ID4ckZzQNw",
        "outputId": "e380f67d-c5b6-47dc-b2ea-8b9499e4bcda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement os (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for os\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pprint (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pprint\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting argparse\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Installing collected packages: argparse\n",
            "Successfully installed argparse-1.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.36.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install os\n",
        "!pip install pprint\n",
        "!pip install argparse\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "\n",
        "import os\n",
        "import pprint\n",
        "import argparse\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl-7cTra1J7O"
      },
      "source": [
        "### Get a tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JIRBsLLC1Prr"
      },
      "outputs": [],
      "source": [
        "def get_tokenizer_gpt(arch):\n",
        "    \"\"\"\n",
        "    If args.tokenizer_merges_file is given, return a tokenizer that uses that merges_file.\n",
        "    In the paper, we use this to restrict models to ingest and outuput digits. For example:\n",
        "\n",
        "    >>> tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\", merges_file=\"merges_gpt2_single_digit_numbers.txt\")\n",
        "    >>> tokenizer_old = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    >>> tokenizer.encode(\"1\")\n",
        "    [16]\n",
        "    >>> tokenizer_old.encode(\"1\")\n",
        "    [16]\n",
        "    >>> tokenizer.encode(\"2\")\n",
        "    [17]\n",
        "    >>> tokenizer_old.encode(\"12\")\n",
        "    [1065]\n",
        "    >>> tokenizer.encode(\"12\")\n",
        "    [16, 17]\n",
        "    >>> tokenizer.encode(\"HEllo world!\")\n",
        "    [13909, 18798, 995, 0]\n",
        "    >>> tokenizer_old.encode(\"HEllo world!\")\n",
        "    [13909, 18798, 995, 0]\n",
        "    \"\"\"\n",
        "\n",
        "    tokenizer = transformers.GPT2Tokenizer.from_pretrained(arch)\n",
        "    return tokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2bANpc607oq"
      },
      "source": [
        "### Get the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZeR_lpVh1B4n"
      },
      "outputs": [],
      "source": [
        "def get_dataset(mathematica_dataroot = None, arch = 'gpt2'):\n",
        "    tokenizer = get_tokenizer_gpt(arch)\n",
        "    print(tokenizer)\n",
        "    # print(tokenizer.tokenize(\"1231231234441234 blah dklkjl12490!!@ 2*x + y^k + f(x)\"))  # sanity check\n",
        "\n",
        "\n",
        "    train_data = []\n",
        "\n",
        "    if mathematica_dataroot:\n",
        "      for mathematica_dr in mathematica_dataroot:\n",
        "        print(mathematica_dr)\n",
        "        # Save path to txt file which contains all txt files of math problems and answers for a specific category\n",
        "        # Algebra\n",
        "        find_roots_dir = os.path.join(mathematica_dataroot, \"algebra/find_roots/\")\n",
        "        invert_function_dir = os.path.join(mathematica_dataroot, \"algebra/invert_function/\")\n",
        "\n",
        "        # Calculus\n",
        "        derivatives_dir = os.path.join(mathematica_dataroot, \"calculus/derivatives/\")\n",
        "        integrals_dir = os.path.join(mathematica_dataroot, \"calculus/integrals/\")\n",
        "\n",
        "        # Geometry\n",
        "        polygons_dir = os.path.join(mathematica_dataroot, \"geometry/polygons_train_data/\")\n",
        "        triangles_dir = os.path.join(mathematica_dataroot, \"geometry/triangles_train_data/\")\n",
        "\n",
        "        # Linear Algebra\n",
        "        determinant_dir = os.path.join(mathematica_dataroot, \"linear_algebra/determinant_train_data/\")\n",
        "        orthogonolize_vectors_dir = os.path.join(mathematica_dataroot, \"linear_algebra/orthogonalize_vectors_train_data/\")\n",
        "\n",
        "        flist_find_roots = os.listdir(find_roots_dir)\n",
        "        find_roots_num_files = len(flist_find_roots)\n",
        "\n",
        "        flist_invert_function = os.listdir(invert_function_dir)\n",
        "        invert_function_num_files = len(flist_invert_function)\n",
        "\n",
        "        flist_derivatives = os.listdir(derivatives_dir)\n",
        "        derivatives_num_files = len(flist_derivatives)\n",
        "\n",
        "        flist_integrals = os.listdir(integrals_dir)\n",
        "        integrals_num_files = len(flist_integrals)\n",
        "\n",
        "        flist_polygons = os.listdir(polygons_dir)\n",
        "        polygons_num_files = len(flist_polygons)\n",
        "\n",
        "        flist_triangles = os.listdir(triangles_dir)\n",
        "        triangles_num_files = len(flist_triangles)\n",
        "\n",
        "        flist_determinant = os.listdir(determinant_dir)\n",
        "        determinant_num_files = len(flist_determinant)\n",
        "\n",
        "        flist_orthogonolize_vectors = os.listdir(orthogonolize_vectors_dir)\n",
        "        orthogonolize_vectors_num_files = len(flist_orthogonolize_vectors)\n",
        "\n",
        "        if find_roots_num_files:\n",
        "          train_data.append(MathematicaMathDataset(\n",
        "            dataroot=find_roots_dir,\n",
        "            tokenizer=tokenizer,\n",
        "            max_tokens=384 if arch == 'gpt2-xl' else 1024,\n",
        "            mode='gpt2',\n",
        "          ))\n",
        "\n",
        "        if invert_function_num_files:\n",
        "          train_data.append(MathematicaMathDataset(\n",
        "            dataroot=invert_function_dir,\n",
        "            tokenizer=tokenizer,\n",
        "            max_tokens=384 if arch == 'gpt2-xl' else 1024,\n",
        "            mode='gpt2',\n",
        "          ))\n",
        "\n",
        "        if derivatives_num_files:\n",
        "          train_data.append(MathematicaMathDataset(\n",
        "            dataroot=derivatives_dir,\n",
        "            tokenizer=tokenizer,\n",
        "            max_tokens=384 if arch == 'gpt2-xl' else 1024,\n",
        "            mode='gpt2'\n",
        "          ))\n",
        "\n",
        "        if integrals_num_files:\n",
        "          train_data.append(MathematicaMathDataset(\n",
        "            dataroot=integrals_dir,\n",
        "            tokenizer=tokenizer,\n",
        "            max_tokens=384 if arch == 'gpt2-xl' else 1024,\n",
        "            mode='gpt2'\n",
        "          ))\n",
        "\n",
        "        if polygons_num_files:\n",
        "          train_data.append(MathematicaMathDataset(\n",
        "            dataroot=polygons_dir,\n",
        "            tokenizer=tokenizer,\n",
        "            max_tokens=384 if arch == 'gpt2-xl' else 1024,\n",
        "            mode='gpt2'\n",
        "          ))\n",
        "\n",
        "        if triangles_num_files:\n",
        "          train_data.append(MathematicaMathDataset(\n",
        "            dataroot=triangles_dir,\n",
        "            tokenizer=tokenizer,\n",
        "            max_tokens=384 if arch == 'gpt2-xl' else 1024,\n",
        "            mode='gpt2',\n",
        "          ))\n",
        "\n",
        "        if determinant_num_files:\n",
        "          train_data.append(MathematicaMathDataset(\n",
        "            dataroot=determinant_dir,\n",
        "            tokenizer=tokenizer,\n",
        "            max_tokens=384 if arch == 'gpt2-xl' else 1024,\n",
        "            mode='gpt2',\n",
        "          ))\n",
        "\n",
        "        if orthogonolize_vectors_num_files:\n",
        "          train_data.append(MathematicaMathDataset(\n",
        "            dataroot=orthogonolize_vectors_dir,\n",
        "            tokenizer=tokenizer,\n",
        "            max_tokens=384 if arch == 'gpt2-xl' else 1024,\n",
        "            mode='gpt2',\n",
        "          ))\n",
        "\n",
        "        # Print the sizes of each dataset, useful for weighting\n",
        "        for dset in train_data:\n",
        "          print(f\"{dset.__class__.__name__}: __len__ = {len(dset)}\")\n",
        "\n",
        "        return torch.utils.data.ConcatDataset(train_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jqee8vCGz4XO"
      },
      "source": [
        "### GPT2 Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "inmORgIEz6dx"
      },
      "outputs": [],
      "source": [
        "class GPT2Trainer(transformers.Trainer):\n",
        "    def create_optimizer_and_scheduler(self, num_training_steps: int):\n",
        "        \"\"\"\n",
        "        Setup the optimizer and the learning rate scheduler.\n",
        "        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n",
        "        Trainer's init through :obj:`optimizers`, or subclass and override this method in a subclass.\n",
        "        \"\"\"\n",
        "        if self.optimizer is None:\n",
        "            print(\"Making AdamW Optimizer\")\n",
        "            no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "            optimizer_grouped_parameters = [\n",
        "                {\n",
        "                    \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "                    \"weight_decay\": self.args.weight_decay,\n",
        "                },\n",
        "                {\n",
        "                    \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "                    \"weight_decay\": 0.0,\n",
        "                },\n",
        "            ]\n",
        "            self.optimizer = torch.optim.AdamW(\n",
        "                optimizer_grouped_parameters,\n",
        "                lr=self.args.learning_rate,\n",
        "                betas=(self.args.adam_beta1, self.args.adam_beta2),\n",
        "                eps=self.args.adam_epsilon,\n",
        "            )\n",
        "\n",
        "        if self.lr_scheduler is None:\n",
        "\n",
        "            if self.args.warmup_steps == -1:\n",
        "                print(\"Using constant LR\")\n",
        "                self.lr_scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lambda steps: 1.0)\n",
        "            else:\n",
        "                print(\"Using Linear warmup LR\")\n",
        "                self.lr_scheduler = self.get_linear_schedule_with_warmup(\n",
        "                    self.optimizer, num_warmup_steps=self.args.warmup_steps, num_training_steps=num_training_steps\n",
        "                )\n",
        "\n",
        "    @staticmethod\n",
        "    def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, last_epoch=-1):\n",
        "        \"\"\"\n",
        "        Linear warmup from 0 to max lr, then linear decay from max_lr to 0.1*max_lr\n",
        "        As done in https://arxiv.org/pdf/2010.14701.pdf\n",
        "        \"\"\"\n",
        "\n",
        "        def lr_lambda(current_step: int):\n",
        "            if current_step < num_warmup_steps:\n",
        "                return float(current_step) / float(max(1, num_warmup_steps))\n",
        "            min_lr_multiplier = 0.1\n",
        "            return max(\n",
        "                min_lr_multiplier,\n",
        "                ((1 - min_lr_multiplier) * float(num_training_steps - current_step) / float(\n",
        "                    max(1, num_training_steps - num_warmup_steps))) + min_lr_multiplier\n",
        "            )\n",
        "\n",
        "        return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvKPMMEl0i0g"
      },
      "source": [
        "### Run the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIXsExxOq4yx",
        "outputId": "941850a1-f267-44c4-bdc0-be95f64cfb88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.25.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.36.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U accelerate\n",
        "!pip install -U transformers\n",
        "import accelerate\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "503iiFuO0mSa"
      },
      "outputs": [],
      "source": [
        "def run_training(save_steps=0, tpu_num_cores=None, grad_acc_steps=4, batch_size_per_replica=4, load=None, arch='gpt2', save_dir='checkpoints/TEMP', epochs=2, lr=5e-5, weight_decay=0.05, lr_warmup_steps=1, log_freq=5, dataloader_num_workers=1, local_rank=-1, train_data=None):\n",
        "    save_dir = os.path.join(save_dir, datetime.now().strftime(\"%m-%d-%Y__%H:%M:%S\"))\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    if not save_steps:\n",
        "        # Save every epoch\n",
        "        if not tpu_num_cores:\n",
        "            save_steps = len(train_data)\n",
        "            # print(\"mps_is_available = \" + str(torch.backends.mps.is_available()))  # the MacOS is higher than 12.3+\n",
        "            # print(\"mps_is_built = \" + str(torch.backends.mps.is_built()))  # MPS is activated\n",
        "\n",
        "            # save_steps = int(save_steps / torch.cuda.device_count())\n",
        "            save_steps = int(save_steps / grad_acc_steps)\n",
        "            save_steps = int(save_steps / batch_size_per_replica)\n",
        "        else:\n",
        "            save_steps = len(train_data)\n",
        "            save_steps = int(save_steps / 8)  # 8 TPU cores is constant for now.\n",
        "            save_steps = int(save_steps / grad_acc_steps)\n",
        "            save_steps = int(save_steps / batch_size_per_replica)\n",
        "    else:\n",
        "        save_steps = save_steps\n",
        "\n",
        "    print(\"Save Steps = \", save_steps)\n",
        "\n",
        "    ## Checkpoint Loading ########################################################\n",
        "    if load:\n",
        "        model = transformers.GPT2LMHeadModel.from_pretrained(load)\n",
        "        print(f\"Loaded model from {load}\")\n",
        "    else:\n",
        "        model = transformers.GPT2LMHeadModel.from_pretrained(arch)\n",
        "\n",
        "    start_epoch = 0\n",
        "    start_iteration = 0\n",
        "\n",
        "    ## Dataloading ########################################################\n",
        "    train_data.start_iteration = start_iteration\n",
        "\n",
        "    ## Start Loop ########################################################\n",
        "    print(f\"Setting up Trainer\")\n",
        "\n",
        "    training_args = transformers.TrainingArguments(\n",
        "        output_dir=save_dir,\n",
        "        overwrite_output_dir=False,\n",
        "\n",
        "        do_train=True,\n",
        "        do_eval=False,\n",
        "        do_predict=True,\n",
        "        evaluation_strategy='no',\n",
        "        eval_steps=0,\n",
        "\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=batch_size_per_replica,\n",
        "        gradient_accumulation_steps=grad_acc_steps,\n",
        "\n",
        "        learning_rate=lr,\n",
        "        weight_decay=weight_decay,\n",
        "        warmup_steps=lr_warmup_steps,\n",
        "        max_grad_norm=100000.0,  # Essentially disable gradient clipping\n",
        "\n",
        "        logging_dir=save_dir,\n",
        "        logging_first_step=True,\n",
        "        logging_steps=log_freq,\n",
        "        save_steps=save_steps,\n",
        "        save_total_limit=10,  # Only save the last epoch\n",
        "\n",
        "        dataloader_drop_last=True,\n",
        "        dataloader_num_workers=dataloader_num_workers,\n",
        "\n",
        "        local_rank=local_rank,\n",
        "        tpu_num_cores=tpu_num_cores,\n",
        "    )\n",
        "\n",
        "    trainer = GPT2Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_data,\n",
        "    )\n",
        "    trainer.remove_callback(transformers.integrations.TensorBoardCallback)\n",
        "    # trainer.add_callback(CustomTensorBoardCallback())\n",
        "\n",
        "    print(f\"STARTING TRAINING. save_steps={save_steps}\")\n",
        "    trainer.train()\n",
        "\n",
        "    trainer.save_model(os.path.join(save_dir, \"final_checkpoint\"))\n",
        "    print(\"Finished\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xd3hcwuU1weq"
      },
      "source": [
        "### Specify additional args and start the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810,
          "referenced_widgets": [
            "620f7aa341d7457a94acf318373b2fe5",
            "e671810a82cb4b79a792d82f5de1d23e",
            "743501cd3bbf409e8c831fc13b68a3a0",
            "a3d266371fc747cd8002131f94d2e375",
            "355dcee313d3455a94da00a333cfc157",
            "3ecf8a2b0fd14ff7b06b9138456315b0",
            "68047da96ab54cd7a25146af2fd31a30",
            "6398d87a52864fe78875b9abfc2bb204",
            "52a49cba847d4b57b80459b7b4ef5ad3",
            "9c23e5e523e54c8ba13cc93214df50db",
            "7f809c0e88dc4c998118c1387bbc850c",
            "0e9671695e2d47c7b71e19c5027a9c65",
            "dc9e979d3e1a4a4585626b0e1bf6bd93",
            "68177b9a6a1f431eb0c31a98394fc167",
            "c08eb1efd25d4366bbda0027e0d8f46a",
            "09c27c19fa3748cb8d8099343636627e",
            "e7ad6d7b97ae4273b0493c89c2436bf5",
            "590083fe0d224f328730e23c56dacf4e",
            "5f3e525550914c498ea5ea2b9ec62718",
            "83c7710d081c4cdb849ebd5527761896",
            "8e4bc912917242ae837873205818ff8b",
            "c99748ec6570405199fd624d54a71709",
            "b4040a35e0c84c21a44c7a04aa1dca46",
            "e89e4152d11a443f9f58ee6ad0cc06fc",
            "285c34e5a6264d21b6770484c2a67998",
            "2e143f17c8fa4837b281466999b2af74",
            "15a76216a7fe4cafb480d61dc42cd799",
            "631b3650a7d0435dac43d661fe2053b8",
            "6544b352421e40aa8d09800f407a2a27",
            "af1b8389b5f042a98ee1edf6dde64c85",
            "e0e62ba67fdc4c58bef1fbb7c75f82cc",
            "5293d57f31174850ae2638914d70d941",
            "2a9034099617422b899bc2a016ab53df",
            "df23752850d4459c96110c74a5e13a7f",
            "56e136fc1213428c96d4bdff62d7d3d2",
            "d87086a2dde14609aacfd7952ac5aaaf",
            "2fc1708381bc4728ba71d161add34e59",
            "4be062f81713495e9fa1e4aa36b5ae6c",
            "a52aa6b4351142f6884bf5f8f1f70801",
            "293c4bd469ba4e999619d6cf10020189",
            "95b9c71fd96c4bfca3e41261527aed5b",
            "c08812d86a874cbea910fb7224a32088",
            "be822cd6861247efb544e4d22b4b0ef0",
            "ad5efc76bab44278b036875f7ddd6d07"
          ]
        },
        "id": "l5pksXQb19E2",
        "outputId": "28a4e6f7-3fe3-4df7-9fdd-e62aa788d8cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "620f7aa341d7457a94acf318373b2fe5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e9671695e2d47c7b71e19c5027a9c65"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4040a35e0c84c21a44c7a04aa1dca46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df23752850d4459c96110c74a5e13a7f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2Tokenizer(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "}\n",
            "/\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-06ed1ae4e47c>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-06ed1ae4e47c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m######### Start training ###############################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmathematica_dataroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/tmp/sample_data/train_data/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-dee93fef0ed4>\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(mathematica_dataroot, arch)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0morthogonolize_vectors_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmathematica_dataroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"linear_algebra/orthogonalize_vectors_train_data/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mflist_find_roots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_roots_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mfind_roots_num_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflist_find_roots\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/sample_data/train_data/algebra/find_roots/'"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    ######### Start training ###############################################################\n",
        "\n",
        "    train_data = get_dataset(mathematica_dataroot='/tmp/sample_data/train_data/')\n",
        "    run_training(train_data=train_data)\n",
        "\n",
        "\n",
        "def _mp_fn(index):\n",
        "    # For xla_spawn (TPUs)\n",
        "    main()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "3UqmXtDOxI_2"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPX4oR6MFdDa6LY//cZpAh7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "620f7aa341d7457a94acf318373b2fe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e671810a82cb4b79a792d82f5de1d23e",
              "IPY_MODEL_743501cd3bbf409e8c831fc13b68a3a0",
              "IPY_MODEL_a3d266371fc747cd8002131f94d2e375"
            ],
            "layout": "IPY_MODEL_355dcee313d3455a94da00a333cfc157"
          }
        },
        "e671810a82cb4b79a792d82f5de1d23e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ecf8a2b0fd14ff7b06b9138456315b0",
            "placeholder": "​",
            "style": "IPY_MODEL_68047da96ab54cd7a25146af2fd31a30",
            "value": "vocab.json: 100%"
          }
        },
        "743501cd3bbf409e8c831fc13b68a3a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6398d87a52864fe78875b9abfc2bb204",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_52a49cba847d4b57b80459b7b4ef5ad3",
            "value": 1042301
          }
        },
        "a3d266371fc747cd8002131f94d2e375": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c23e5e523e54c8ba13cc93214df50db",
            "placeholder": "​",
            "style": "IPY_MODEL_7f809c0e88dc4c998118c1387bbc850c",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 13.8MB/s]"
          }
        },
        "355dcee313d3455a94da00a333cfc157": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ecf8a2b0fd14ff7b06b9138456315b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68047da96ab54cd7a25146af2fd31a30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6398d87a52864fe78875b9abfc2bb204": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52a49cba847d4b57b80459b7b4ef5ad3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9c23e5e523e54c8ba13cc93214df50db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f809c0e88dc4c998118c1387bbc850c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e9671695e2d47c7b71e19c5027a9c65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dc9e979d3e1a4a4585626b0e1bf6bd93",
              "IPY_MODEL_68177b9a6a1f431eb0c31a98394fc167",
              "IPY_MODEL_c08eb1efd25d4366bbda0027e0d8f46a"
            ],
            "layout": "IPY_MODEL_09c27c19fa3748cb8d8099343636627e"
          }
        },
        "dc9e979d3e1a4a4585626b0e1bf6bd93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7ad6d7b97ae4273b0493c89c2436bf5",
            "placeholder": "​",
            "style": "IPY_MODEL_590083fe0d224f328730e23c56dacf4e",
            "value": "merges.txt: 100%"
          }
        },
        "68177b9a6a1f431eb0c31a98394fc167": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f3e525550914c498ea5ea2b9ec62718",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_83c7710d081c4cdb849ebd5527761896",
            "value": 456318
          }
        },
        "c08eb1efd25d4366bbda0027e0d8f46a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e4bc912917242ae837873205818ff8b",
            "placeholder": "​",
            "style": "IPY_MODEL_c99748ec6570405199fd624d54a71709",
            "value": " 456k/456k [00:00&lt;00:00, 10.8MB/s]"
          }
        },
        "09c27c19fa3748cb8d8099343636627e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7ad6d7b97ae4273b0493c89c2436bf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "590083fe0d224f328730e23c56dacf4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f3e525550914c498ea5ea2b9ec62718": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83c7710d081c4cdb849ebd5527761896": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e4bc912917242ae837873205818ff8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c99748ec6570405199fd624d54a71709": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4040a35e0c84c21a44c7a04aa1dca46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e89e4152d11a443f9f58ee6ad0cc06fc",
              "IPY_MODEL_285c34e5a6264d21b6770484c2a67998",
              "IPY_MODEL_2e143f17c8fa4837b281466999b2af74"
            ],
            "layout": "IPY_MODEL_15a76216a7fe4cafb480d61dc42cd799"
          }
        },
        "e89e4152d11a443f9f58ee6ad0cc06fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_631b3650a7d0435dac43d661fe2053b8",
            "placeholder": "​",
            "style": "IPY_MODEL_6544b352421e40aa8d09800f407a2a27",
            "value": "tokenizer.json: 100%"
          }
        },
        "285c34e5a6264d21b6770484c2a67998": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af1b8389b5f042a98ee1edf6dde64c85",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e0e62ba67fdc4c58bef1fbb7c75f82cc",
            "value": 1355256
          }
        },
        "2e143f17c8fa4837b281466999b2af74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5293d57f31174850ae2638914d70d941",
            "placeholder": "​",
            "style": "IPY_MODEL_2a9034099617422b899bc2a016ab53df",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 18.5MB/s]"
          }
        },
        "15a76216a7fe4cafb480d61dc42cd799": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "631b3650a7d0435dac43d661fe2053b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6544b352421e40aa8d09800f407a2a27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af1b8389b5f042a98ee1edf6dde64c85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0e62ba67fdc4c58bef1fbb7c75f82cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5293d57f31174850ae2638914d70d941": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a9034099617422b899bc2a016ab53df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df23752850d4459c96110c74a5e13a7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_56e136fc1213428c96d4bdff62d7d3d2",
              "IPY_MODEL_d87086a2dde14609aacfd7952ac5aaaf",
              "IPY_MODEL_2fc1708381bc4728ba71d161add34e59"
            ],
            "layout": "IPY_MODEL_4be062f81713495e9fa1e4aa36b5ae6c"
          }
        },
        "56e136fc1213428c96d4bdff62d7d3d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a52aa6b4351142f6884bf5f8f1f70801",
            "placeholder": "​",
            "style": "IPY_MODEL_293c4bd469ba4e999619d6cf10020189",
            "value": "config.json: 100%"
          }
        },
        "d87086a2dde14609aacfd7952ac5aaaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95b9c71fd96c4bfca3e41261527aed5b",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c08812d86a874cbea910fb7224a32088",
            "value": 665
          }
        },
        "2fc1708381bc4728ba71d161add34e59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be822cd6861247efb544e4d22b4b0ef0",
            "placeholder": "​",
            "style": "IPY_MODEL_ad5efc76bab44278b036875f7ddd6d07",
            "value": " 665/665 [00:00&lt;00:00, 58.2kB/s]"
          }
        },
        "4be062f81713495e9fa1e4aa36b5ae6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a52aa6b4351142f6884bf5f8f1f70801": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "293c4bd469ba4e999619d6cf10020189": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95b9c71fd96c4bfca3e41261527aed5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c08812d86a874cbea910fb7224a32088": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "be822cd6861247efb544e4d22b4b0ef0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad5efc76bab44278b036875f7ddd6d07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}