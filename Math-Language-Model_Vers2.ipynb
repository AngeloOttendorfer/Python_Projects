{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8XENd2WVNMVtfNt4h8ZFE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AngeloOttendorfer/Python_Projects/blob/master/Math-Language-Model_Vers2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "rCHclPfcq7DR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {
        "id": "mId7wCNhtSE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def last_boxed_only(sample):\n",
        "    \"\"\"\n",
        "    Given a (q,a) sample, filter the answers so that they only contain\n",
        "    the last \\boxed{...} or \\fbox{...} element\n",
        "    \"\"\"\n",
        "    q, a = sample\n",
        "    a = last_boxed_only_string(a)\n",
        "    if a == None:\n",
        "        return None\n",
        "    return (q, a)\n",
        "\n",
        "def last_boxed_only_string(string):\n",
        "    idx = string.rfind(\"\\\\boxed\")\n",
        "    if idx < 0:\n",
        "        idx = string.rfind(\"\\\\fbox\")\n",
        "        if idx < 0:\n",
        "            return None\n",
        "\n",
        "    i = idx\n",
        "    right_brace_idx = None\n",
        "    num_left_braces_open = 0\n",
        "    while i < len(string):\n",
        "        if string[i] == \"{\":\n",
        "            num_left_braces_open += 1\n",
        "        if string[i] == \"}\":\n",
        "            num_left_braces_open -= 1\n",
        "            if num_left_braces_open == 0:\n",
        "                right_brace_idx = i\n",
        "                break\n",
        "        i += 1\n",
        "\n",
        "    if right_brace_idx == None:\n",
        "        retval = None\n",
        "    else:\n",
        "        retval = string[idx:right_brace_idx + 1]\n",
        "\n",
        "    return retval\n",
        "\n",
        "def only_until_first_boxed_from_tokens(string, tokens):\n",
        "    idx = string.find(\"\\\\boxed\")\n",
        "    if idx < 0:\n",
        "        idx = string.find(\"\\\\fbox\")\n",
        "        if idx < 0:\n",
        "            return None\n",
        "\n",
        "    cum_length = 0\n",
        "    for i, t in enumerate(tokens):\n",
        "        cum_length += len(t)\n",
        "        if cum_length >= idx:\n",
        "            break\n",
        "\n",
        "    return tokens[:i]\n",
        "\n",
        "def clean_numbers(sample):\n",
        "    if not sample:\n",
        "        return None\n",
        "    new_sample = list()\n",
        "    for s in sample:\n",
        "        new_sample.append(_clean_numbers(s))\n",
        "\n",
        "    return tuple(new_sample)\n",
        "\n",
        "def _clean_numbers(string):\n",
        "    \"\"\"\n",
        "    Clean Numbers in the given string\n",
        "\n",
        "    >>> _clean_numbers(None, \"Hello 123\")\n",
        "    'Hello 123'\n",
        "    >>> _clean_numbers(None, \"Hello 1234\")\n",
        "    'Hello 1,234'\n",
        "    >>> _clean_numbers(None, \"Hello 1234324asdasd\")\n",
        "    'Hello 1,234,324asdasd'\n",
        "    \"\"\"\n",
        "    num_prev_digits = 0\n",
        "    new_string = \"\"\n",
        "    for i, c in enumerate(string):\n",
        "        # isdigit() doesnt work here because of weird unicode chars.\n",
        "        if c in {'1', '2', '3', '4', '5', '6', '7', '8', '9', '0'}:\n",
        "            num_prev_digits += 1\n",
        "        else:\n",
        "            if num_prev_digits > 3:\n",
        "                # Some fixing\n",
        "                string_number = new_string[-num_prev_digits:]\n",
        "                new_string = new_string[:-num_prev_digits] + \"{0:,}\".format(int(string_number))\n",
        "            num_prev_digits = 0\n",
        "        new_string += c\n",
        "\n",
        "    if num_prev_digits > 3:\n",
        "        # Some fixing\n",
        "        string_number = new_string[-num_prev_digits:]\n",
        "        new_string = new_string[:-num_prev_digits] + \"{0:,}\".format(int(string_number))\n",
        "\n",
        "    return new_string"
      ],
      "metadata": {
        "id": "LTmYjikGtVRD"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base Math Dataset"
      ],
      "metadata": {
        "id": "WqIDQyuPrBAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### import necessary modules"
      ],
      "metadata": {
        "id": "oSJ_Um81rMij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch\n",
        "# !pip install torch.nn.functional\n",
        "# !pip install random\n",
        "# !pip install os\n",
        "# !pip install time\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "FDw0-PpVrRQn"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation"
      ],
      "metadata": {
        "id": "2yj-w5M_uUxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseMathDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Configurable AMPS Dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataroot, tokenizer, max_tokens, mode, mode_answer='default', len_multiplier=1.0, packing=None,\n",
        "                 randomize=None, pack_end=None, clean_numbers=True, latex_mask=False, peek_fraction=(0.1, 1.0)):\n",
        "        self.dataroot = dataroot\n",
        "        self.tokenizer = tokenizer  # Set in run_training(), not in dataset creation\n",
        "        self.max_tokens = max_tokens\n",
        "        self.mode = mode\n",
        "        self.mode_answer = mode_answer  # Used in subclass\n",
        "        self.len_multiplier = len_multiplier\n",
        "        self.clean_numbers = clean_numbers\n",
        "        self.latex_mask = latex_mask\n",
        "        self.peek_fraction = peek_fraction\n",
        "\n",
        "        if self.mode in {'gpt2'}:\n",
        "            self.clean_sample = self.clean_filter_sample_gpt\n",
        "            self.packing = True\n",
        "            self.randomize = True\n",
        "            self.include_fnames = True\n",
        "            self.pack_end = True\n",
        "        elif self.mode in {'gpt2-eval'}:\n",
        "            self.clean_sample = self.clean_filter_sample_gpt_eval\n",
        "            self.packing = True\n",
        "            self.randomize = False\n",
        "            self.include_fnames = True\n",
        "            self.pack_end = True\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "        if packing != None:\n",
        "            print(\"Overriding packing to be\", packing)\n",
        "            self.packing = packing\n",
        "        if randomize != None:\n",
        "            print(\"Overriding randomize to be\", randomize)\n",
        "            self.randomize = randomize\n",
        "        if pack_end != None:\n",
        "            print(\"Overriding pack_end to be\", pack_end)\n",
        "            self.pack_end = pack_end\n",
        "\n",
        "        self.initialize()\n",
        "\n",
        "        self.bad_fnames = set()\n",
        "        self.i = 0\n",
        "\n",
        "    def initialize(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def __len__(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # Each worker needs a different seed....\n",
        "        random.seed(os.getpid() + time.time() + random.random())\n",
        "\n",
        "        # Sampling with replacement.\n",
        "        # We need to pack random elements to get close to self.max_tokens\n",
        "        curr_input_ids = []\n",
        "        curr_label_ids = []\n",
        "        curr_fnames = []\n",
        "        num_samples = 0\n",
        "        while len(curr_input_ids) + 1 <= self.max_tokens and len(curr_label_ids) + 1 <= self.max_tokens:\n",
        "            # print(\"curr_input_ids: \" + str(curr_input_ids))\n",
        "            # print(\"curr_label_ids: \" + str(curr_label_ids))\n",
        "            # print(\"curr_fnames: \" + str(curr_fnames))\n",
        "            curr_sample, fname = self.get_random_sample()\n",
        "            # print(\"current_sample: \" + str(curr_sample))\n",
        "            # print(fname)\n",
        "            if curr_sample is None:\n",
        "                # This only happens in eval modes\n",
        "                return {\n",
        "                    \"input_ids\": torch.zeros([self.max_tokens]),\n",
        "                    \"labels\": torch.zeros([self.max_tokens]),\n",
        "                    \"fnames\": [fname]\n",
        "                }\n",
        "\n",
        "            if not self.pack_end and (\n",
        "                    (len(curr_input_ids) + 1 + len(curr_sample['input_ids_list']) > self.max_tokens) or\n",
        "                    (len(curr_label_ids) + 1 + len(curr_sample['label_ids_list']) > self.max_tokens)\n",
        "            ):\n",
        "                # Do not include curr_sample if either the input_ids or the label_ids will run off the end.\n",
        "                break\n",
        "\n",
        "            # Add curr_sample to the current inputs and labels\n",
        "            # print(\"input_ids_list: \" + str(curr_sample['input_ids_list']))\n",
        "            curr_input_ids.extend(curr_sample['input_ids_list'])\n",
        "            curr_label_ids.extend(curr_sample['label_ids_list'])\n",
        "            curr_fnames.append(fname)\n",
        "\n",
        "            num_samples += 1\n",
        "\n",
        "            # Break on the first iteration if we don't want to do packing.\n",
        "            if not self.packing:\n",
        "                break\n",
        "\n",
        "        input_ids = torch.LongTensor(curr_input_ids)\n",
        "        label_ids = torch.LongTensor(curr_label_ids)\n",
        "\n",
        "        # Sanity check\n",
        "        if 'eval' not in self.mode:\n",
        "            assert len(curr_input_ids) == len(curr_label_ids)\n",
        "\n",
        "        input_ids = input_ids[:self.max_tokens]\n",
        "        label_ids = label_ids[:self.max_tokens]\n",
        "\n",
        "        if len(curr_input_ids) < self.max_tokens and 'eval' not in self.mode:\n",
        "            # Pad\n",
        "            num_to_pad = self.max_tokens - len(curr_input_ids)\n",
        "            input_ids = F.pad(input_ids, [0, num_to_pad], mode='constant', value=self.tokenizer.pad_token_id)\n",
        "\n",
        "        if len(curr_label_ids) < self.max_tokens and 'eval' not in self.mode:\n",
        "            num_to_pad = self.max_tokens - len(curr_label_ids)\n",
        "            label_ids = F.pad(label_ids, [0, num_to_pad], mode='constant', value=-100)\n",
        "\n",
        "        # Sanity check\n",
        "        if 'eval' not in self.mode:\n",
        "            assert input_ids.shape[0] == label_ids.shape[\n",
        "                0] == self.max_tokens, f\"{input_ids.shape[0]}, {label_ids.shape[0]}, {self.max_tokens}\"\n",
        "\n",
        "        if self.include_fnames:\n",
        "            return {\n",
        "                \"input_ids\": input_ids,\n",
        "                \"labels\": label_ids,\n",
        "                \"fnames\": curr_fnames\n",
        "            }\n",
        "        else:\n",
        "            # This is the format required by our GPT2Trainer class\n",
        "            return {\n",
        "                \"input_ids\": input_ids,\n",
        "                \"labels\": label_ids\n",
        "            }\n",
        "\n",
        "    def get_random_sample(self):\n",
        "        \"\"\"\n",
        "        Get a full on random sample (used for training)\n",
        "        \"\"\"\n",
        "        random_sample = None\n",
        "        while random_sample is None:\n",
        "            if self.randomize:\n",
        "                q, a, fname = random.choice(self.samples)\n",
        "            else:\n",
        "                q, a, fname = self.samples[self.i]\n",
        "                self.i = (self.i + 1) % len(self.samples)\n",
        "\n",
        "            random_sample = self.clean_sample((q, a))  # q + '\\n' + a  # self.clean_sample((q, a))\n",
        "            # print(\"random_sample: \" + str(random_sample))\n",
        "\n",
        "            if not self.randomize:\n",
        "                break\n",
        "\n",
        "        return random_sample, fname\n",
        "\n",
        "    def clean_filter_sample_gpt(self, sample):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def clean_filter_sample_gpt_eval(self, sample):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def clean_filter_sample_t5(self, sample):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def clean_filter_sample_t5_eval(self, sample):\n",
        "        raise NotImplementedError()\n"
      ],
      "metadata": {
        "id": "OGv0I9SLq_CP"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configurable Mathematica Dataset"
      ],
      "metadata": {
        "id": "FEcedn3BspKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import necessary modules"
      ],
      "metadata": {
        "id": "A5EIu8e3uhrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch\n",
        "# !pip install tqdm\n",
        "# !pip install os\n",
        "import torch\n",
        "import tqdm\n",
        "import os"
      ],
      "metadata": {
        "id": "_E6t2MQfupVA"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Implementation"
      ],
      "metadata": {
        "id": "mGV7vynivyFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MathematicaMathDataset(BaseMathDataset):\n",
        "    \"\"\"Configurable Math Dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(len(self.samples))\n",
        "\n",
        "    def initialize(self):\n",
        "        \"\"\"\n",
        "        Set up self.samples by loading from the dataroot\n",
        "        \"\"\"\n",
        "\n",
        "        with open(self.dataroot, 'r') as fp:\n",
        "            all_filenames = fp.readlines()\n",
        "\n",
        "        print(all_filenames)\n",
        "\n",
        "        print(f\"{self.__class__.__name__}: Loading samples from {len(all_filenames)} files.\")\n",
        "        samples_raw = []\n",
        "        for fname in tqdm(all_filenames):\n",
        "            fname = fname.rstrip()\n",
        "            # print(fname)\n",
        "            # fname = os.path.join(os.path.dirname(os.path.dirname(self.dataroot)), fname[2:])\n",
        "            # print(fname)\n",
        "\n",
        "            if not os.path.isfile(fname):\n",
        "                print(f\"SKIPPING {fname}\")\n",
        "                continue\n",
        "            with open(fname, 'r') as fp:\n",
        "                question = \"\"\n",
        "                answers  = []\n",
        "                reading_question = True\n",
        "                curr_section = \"\"\n",
        "                for line in fp:\n",
        "                    if line == \"Problem:\\n\":\n",
        "                        reading_question = True\n",
        "                    elif line == \"Answer:\\n\":\n",
        "                        if reading_question:\n",
        "                            # curr_section contains Q\n",
        "                            question = curr_section\n",
        "                        else:\n",
        "                            # curr_section contains an A\n",
        "                            answers.append(curr_section)\n",
        "                        curr_section = \"\"\n",
        "                        reading_question = False\n",
        "                    else:\n",
        "                        curr_section += line\n",
        "\n",
        "                # The last answer needs to be recorded.\n",
        "                answers.append(curr_section)\n",
        "\n",
        "            for a in answers:\n",
        "                samples_raw.append((question, a, fname))\n",
        "\n",
        "        # manager = Manager()\n",
        "        # samples_raw = manager.list(samples_raw)\n",
        "        self.samples = samples_raw\n",
        "        del samples_raw\n",
        "\n",
        "        print(f\"{self.__class__.__name__}: Loaded {len(self.samples)} samples.\")\n",
        "        # print(self.samples)\n",
        "\n",
        "    def clean_filter_sample_gpt(self, sample):\n",
        "        \"\"\"\n",
        "        Does the actual tokenization. Should be parallelized because it can be a bit slow.\n",
        "        \"\"\"\n",
        "\n",
        "        if sample == None:\n",
        "            return None\n",
        "\n",
        "        question, answer = sample\n",
        "        if self.clean_numbers:\n",
        "            question = _clean_numbers(question)\n",
        "            answer = _clean_numbers(answer)\n",
        "\n",
        "        if self.mode_answer == 'default':\n",
        "            question_ids     = torch.LongTensor(self.tokenizer.encode(\"\\nQUESTION:\\n\" + question, verbose=False))\n",
        "\n",
        "            sep_ids          = torch.LongTensor(self.tokenizer.encode(\"\\nFINAL ANSWER:\\n\", verbose=False))\n",
        "            answer_ids       = self.tokenizer.encode(answer, verbose=False)\n",
        "            answer_ids.append(self.tokenizer.eos_token_id)\n",
        "            answer_ids       = torch.LongTensor(answer_ids)\n",
        "\n",
        "            # Use full solution\n",
        "            input_ids = torch.cat([\n",
        "                question_ids,\n",
        "                sep_ids,\n",
        "                answer_ids\n",
        "            ], dim=0)\n",
        "\n",
        "            label_ids = torch.cat([\n",
        "                torch.ones_like(question_ids) * -100,\n",
        "                torch.ones_like(sep_ids) * -100,\n",
        "                answer_ids.clone()\n",
        "            ], dim=0)\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "        # Stop early if this Q,A pair is too long\n",
        "        if input_ids.shape[0] > self.max_tokens:\n",
        "            # Print reason for skipping\n",
        "            # print(f\"{self.__class__.__name__} Skipping due to input_ids being too big. input_ids.shape[0] = {input_ids.shape[0]}.\")\n",
        "            return None\n",
        "\n",
        "        input_ids = input_ids.tolist()\n",
        "        label_ids = label_ids.tolist()\n",
        "\n",
        "        return {\n",
        "            'input_ids_list' : input_ids,\n",
        "            'label_ids_list' : label_ids\n",
        "        }\n",
        "\n",
        "    def clean_filter_sample_t5(self, sample):\n",
        "        \"\"\"\n",
        "        Does the actual tokenization. Should be parallelized because it can be a bit slow.\n",
        "        \"\"\"\n",
        "\n",
        "        if sample == None:\n",
        "            return None\n",
        "\n",
        "        question, answer = sample\n",
        "        if self.clean_numbers:\n",
        "            question = _clean_numbers(question)\n",
        "            answer = _clean_numbers(answer)\n",
        "\n",
        "        if self.mode_answer == 'default':\n",
        "            question_ids     = torch.LongTensor(self.tokenizer.encode(\"\\nQUESTION:\\n\" + question + \"\\nFINAL ANSWER:\\n\", verbose=False))\n",
        "            answer_ids       = torch.LongTensor(self.tokenizer.encode(answer, verbose=False))\n",
        "\n",
        "            input_ids = torch.cat([\n",
        "                question_ids,\n",
        "            ], dim=0)\n",
        "\n",
        "            label_ids = torch.cat([\n",
        "                answer_ids\n",
        "            ], dim=0)\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "        # Stop early if this Q,A pair is too long\n",
        "        if input_ids.shape[0] > self.max_tokens:\n",
        "            # Print reason for skipping\n",
        "            # print(f\"{self.__class__.__name__} Skipping due to input_ids being too big. input_ids.shape[0] = {input_ids.shape[0]}.\")\n",
        "            return None\n",
        "\n",
        "        input_ids = input_ids.tolist()\n",
        "        label_ids = label_ids.tolist()\n",
        "\n",
        "        return {\n",
        "            'input_ids_list' : input_ids,\n",
        "            'label_ids_list' : label_ids\n",
        "        }\n"
      ],
      "metadata": {
        "id": "WVr1T27xtDPJ"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BCtvq5gSsnES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MATH Dataset Configuration"
      ],
      "metadata": {
        "id": "XzO0KdTvwMSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### import necessary modules"
      ],
      "metadata": {
        "id": "_nhBVNWhwTMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch\n",
        "# !pip install json\n",
        "# !pip install glob\n",
        "# !pip install random\n",
        "# !pip install numpy\n",
        "\n",
        "import torch\n",
        "import json\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from multiprocessing import Manager"
      ],
      "metadata": {
        "id": "cndWlUKEwW5o"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation"
      ],
      "metadata": {
        "id": "3UqmXtDOxI_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MATHDataset(BaseMathDataset):\n",
        "    \"\"\"Configurable Math Dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(len(self.samples) * self.len_multiplier)\n",
        "\n",
        "    def initialize(self):\n",
        "        \"\"\"\n",
        "        Set up self.samples by loading from the dataroot\n",
        "        \"\"\"\n",
        "        print(self.dataroot)\n",
        "        all_filenames = glob.glob('/Users/angeloottendorfer/Desktop/amps/mathematica/algebra/testdaten_find_roots/*')\n",
        "        print(all_filenames)\n",
        "        samples_raw = []\n",
        "        for fname in all_filenames:\n",
        "            with open(fname, 'r') as fp:\n",
        "                try:\n",
        "                    problem_data = json.load(fp)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading JSON from {fname}\", e)\n",
        "                    raise e\n",
        "            curr_sample_raw = (problem_data['problem'], problem_data['solution'], fname)\n",
        "            for e in curr_sample_raw:\n",
        "                assert e\n",
        "            samples_raw.append(curr_sample_raw)\n",
        "\n",
        "        manager = Manager()\n",
        "        samples_raw = manager.list(samples_raw)\n",
        "        self.samples = samples_raw\n",
        "        del samples_raw\n",
        "\n",
        "        print(f\"{self.__class__.__name__}: Loaded {len(self.samples)} samples.\")\n",
        "\n",
        "    def clean_filter_sample_gpt(self, sample):\n",
        "        \"\"\"\n",
        "        Does the actual tokenization. Should be parallelized because it can be a bit slow.\n",
        "        \"\"\"\n",
        "\n",
        "        if sample == None:\n",
        "            return None\n",
        "\n",
        "        if self.mode_answer == 'peeking_only':\n",
        "            return self.clean_filter_sample_peeking_gpt(sample)\n",
        "        if self.mode_answer == 'mixed_full_and_peeking':\n",
        "            if random.random() < 0.5:\n",
        "                return self.clean_filter_sample_peeking_gpt(sample)\n",
        "            else:\n",
        "                _mode_answer = 'full'\n",
        "        elif self.mode_answer == 'mixed_full_and_nopack_padding':\n",
        "            if random.random() < 0.5:\n",
        "                return self.clean_filter_sample_nopackpadding_gpt(sample)\n",
        "            else:\n",
        "                _mode_answer = 'full'\n",
        "        elif self.mode_answer == 'mixed_final_boxed_and_full':\n",
        "            if random.random() < 0.5:\n",
        "                _mode_answer = 'full'\n",
        "            else:\n",
        "                _mode_answer = 'final_boxed'\n",
        "        elif self.mode_answer == 'full':\n",
        "            _mode_answer = 'full'\n",
        "        elif self.mode_answer == 'final_boxed':\n",
        "            _mode_answer = 'final_boxed'\n",
        "        else:\n",
        "            raise NotImplementedError(f\"self.mode_answer = {self.mode_answer} not recognized.\")\n",
        "\n",
        "        if _mode_answer == 'full':\n",
        "            question, answer = sample\n",
        "\n",
        "            if self.clean_numbers:\n",
        "                question = _clean_numbers(question)\n",
        "                answer = _clean_numbers(answer)\n",
        "\n",
        "            answer_final = last_boxed_only_string(answer)\n",
        "\n",
        "            question_ids = torch.LongTensor(self.tokenizer.encode(\"\\nQUESTION:\\n\" + question, verbose=False))\n",
        "\n",
        "            sep_ids_2 = torch.LongTensor(self.tokenizer.encode(\"\\nFULL SOLUTION:\\n\", verbose=False))\n",
        "            answer_ids = self.tokenizer.encode(answer, verbose=False)\n",
        "            answer_ids.append(self.tokenizer.eos_token_id)\n",
        "            answer_ids = torch.LongTensor(answer_ids)\n",
        "\n",
        "            input_ids = torch.cat([\n",
        "                question_ids,\n",
        "                sep_ids_2,\n",
        "                answer_ids\n",
        "            ], dim=0)\n",
        "\n",
        "            # Only answer_ids contribute to the loss\n",
        "            label_ids = torch.cat([\n",
        "                torch.ones_like(question_ids) * -100,\n",
        "                torch.ones_like(sep_ids_2) * -100,\n",
        "                answer_ids.clone()\n",
        "            ], dim=0)\n",
        "\n",
        "        elif _mode_answer == 'final_boxed':\n",
        "            question, answer = sample\n",
        "\n",
        "            if self.clean_numbers:\n",
        "                question = _clean_numbers(question)\n",
        "                answer = _clean_numbers(answer)\n",
        "            answer_final = last_boxed_only_string(answer)\n",
        "            if not answer_final:\n",
        "                print(\"ERROR FROM\", question, answer)\n",
        "                return None\n",
        "\n",
        "            question_ids = torch.LongTensor(self.tokenizer.encode(\"\\nQUESTION:\\n\" + question, verbose=False))\n",
        "\n",
        "            sep_ids_1 = torch.LongTensor(self.tokenizer.encode(\"\\nFINAL ANSWER:\\n\", verbose=False))\n",
        "            answer_final_ids = self.tokenizer.encode(answer_final, verbose=False)\n",
        "            answer_final_ids.append(self.tokenizer.eos_token_id)\n",
        "            answer_final_ids = torch.LongTensor(answer_final_ids)\n",
        "\n",
        "            input_ids = torch.cat([\n",
        "                question_ids,\n",
        "                sep_ids_1,\n",
        "                answer_final_ids,\n",
        "            ], dim=0)\n",
        "\n",
        "            # Only answer_ids contribute to the loss\n",
        "            label_ids = torch.cat([\n",
        "                torch.ones_like(question_ids) * -100,\n",
        "                torch.ones_like(sep_ids_1) * -100,\n",
        "                answer_final_ids.clone(),\n",
        "            ], dim=0)\n",
        "\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "        # Stop early if this Q,A pair is too long\n",
        "        if input_ids.shape[0] > self.max_tokens:\n",
        "            # Print reason for skipping\n",
        "            # print(f\"Skipping due to input_ids being too big. input_ids.shape[0] = {input_ids.shape[0]}.\")\n",
        "            return None\n",
        "\n",
        "        input_ids = input_ids.tolist()\n",
        "        label_ids = label_ids.tolist()\n",
        "\n",
        "        return {\n",
        "            'input_ids_list': input_ids,\n",
        "            'label_ids_list': label_ids\n",
        "        }\n",
        "\n",
        "    def clean_filter_sample_nopackpadding_gpt(self, sample):\n",
        "\n",
        "        if sample == None:\n",
        "            return None\n",
        "\n",
        "        question, answer = sample\n",
        "\n",
        "        if self.clean_numbers:\n",
        "            question = _clean_numbers(question)\n",
        "            answer = _clean_numbers(answer)\n",
        "\n",
        "        answer_final = last_boxed_only_string(answer)\n",
        "\n",
        "        question_ids = torch.LongTensor(self.tokenizer.encode(\"\\nQUESTION:\\n\" + question, verbose=False))\n",
        "        sep_ids = torch.LongTensor(self.tokenizer.encode(\"\\nFINAL ANSWER:\\n\", verbose=False))\n",
        "        final_answer_ids = torch.LongTensor(self.tokenizer.encode(answer_final, verbose=False))\n",
        "\n",
        "        # Stop early if this Q,A pair is too long\n",
        "        num_to_pad = 32\n",
        "        padding_tensor = torch.ones((num_to_pad)) * 220  # 220 is the token for space in the case of GPT2 models\n",
        "\n",
        "        input_ids = torch.cat([\n",
        "            question_ids,\n",
        "            padding_tensor,\n",
        "            sep_ids,\n",
        "            final_answer_ids\n",
        "        ], dim=0)\n",
        "\n",
        "        # Only answer_ids contribute to the loss\n",
        "        label_ids = torch.cat([\n",
        "            torch.ones_like(question_ids) * -100,\n",
        "            torch.ones_like(padding_tensor) * -100,\n",
        "            torch.ones_like(sep_ids) * -100,\n",
        "            final_answer_ids.clone()\n",
        "        ], dim=0)\n",
        "\n",
        "        input_ids = input_ids.tolist()\n",
        "        label_ids = label_ids.tolist()\n",
        "\n",
        "        return {\n",
        "            'input_ids_list': input_ids,\n",
        "            'label_ids_list': label_ids\n",
        "        }\n",
        "\n",
        "    def clean_filter_sample_nopackpadding_gpt_eval(self, sample):\n",
        "\n",
        "        if sample == None:\n",
        "            return None\n",
        "\n",
        "        question, answer = sample\n",
        "\n",
        "        if self.clean_numbers:\n",
        "            question = _clean_numbers(question)\n",
        "            answer = _clean_numbers(answer)\n",
        "\n",
        "        answer_final = last_boxed_only_string(answer)\n",
        "\n",
        "        question_ids = torch.LongTensor(self.tokenizer.encode(\"\\nQUESTION:\\n\" + question, verbose=False))\n",
        "        sep_ids = torch.LongTensor(self.tokenizer.encode(\"\\nFINAL ANSWER:\\n\", verbose=False))\n",
        "        final_answer_ids = torch.LongTensor(self.tokenizer.encode(answer_final, verbose=False))\n",
        "\n",
        "        num_to_pad = 32\n",
        "        padding_tensor = torch.ones((num_to_pad)) * 220  # 220 is the token for space in the case of GPT2 models\n",
        "\n",
        "        input_ids = torch.cat([\n",
        "            question_ids,\n",
        "            padding_tensor,\n",
        "            sep_ids,\n",
        "        ], dim=0)\n",
        "\n",
        "        # Only answer_ids contribute to the loss\n",
        "        label_ids = torch.cat([\n",
        "            final_answer_ids.clone()\n",
        "        ], dim=0)\n",
        "\n",
        "        # Stop early if this Q,A pair is too long\n",
        "        if input_ids.shape[0] + label_ids.shape[0] > self.max_tokens:\n",
        "            # Print reason for skipping\n",
        "            # print(f\"Skipping due to input_ids being too big. input_ids.shape[0] = {input_ids.shape[0]}.\")\n",
        "            return None\n",
        "\n",
        "        input_ids = input_ids.tolist()\n",
        "        label_ids = label_ids.tolist()\n",
        "\n",
        "        return {\n",
        "            'input_ids_list': input_ids,\n",
        "            'label_ids_list': label_ids\n",
        "        }\n",
        "\n",
        "    def clean_filter_sample_peeking_gpt(self, sample):\n",
        "        \"\"\"\n",
        "        Does the actual tokenization. Should be parallelized because it can be a bit slow.\n",
        "        \"\"\"\n",
        "\n",
        "        if sample == None:\n",
        "            return None\n",
        "\n",
        "        question, answer = sample\n",
        "\n",
        "        if self.clean_numbers:\n",
        "            question = _clean_numbers(question)\n",
        "            answer = _clean_numbers(answer)\n",
        "\n",
        "        answer_final = last_boxed_only_string(answer)\n",
        "\n",
        "        question_ids = torch.LongTensor(\n",
        "            self.tokenizer.encode(\"\\nQUESTION:\\n\" + question + \"\\nFULL SOLUTION:\\n\", verbose=False))\n",
        "        answer_ids = self.tokenizer.tokenize(answer)\n",
        "        answer_ids = only_until_first_boxed_from_tokens(answer, answer_ids)\n",
        "        answer_ids = torch.LongTensor(self.tokenizer.encode(answer_ids, verbose=False))\n",
        "\n",
        "        # Take a fraction\n",
        "        if isinstance(self.peek_fraction, tuple):\n",
        "            final_idx = int(len(answer_ids) * random.uniform(*self.peek_fraction))\n",
        "        else:\n",
        "            final_idx = int(len(answer_ids) * self.peek_fraction)\n",
        "\n",
        "        # # Override peeking fraction\n",
        "        # final_idx = int(len(answer_ids) * np.random.choice([0.25, 0.5, 0.75, 1.0], p=[1/6, 1/6, 1/3, 1/3]))\n",
        "\n",
        "        answer_ids = answer_ids[:final_idx]\n",
        "\n",
        "        sep_ids = torch.LongTensor(self.tokenizer.encode(\"\\nFINAL ANSWER:\\n\", verbose=False))\n",
        "        final_answer_ids = torch.LongTensor(self.tokenizer.encode(answer_ids[final_idx:]))\n",
        "\n",
        "        input_ids = torch.cat([\n",
        "            question_ids,\n",
        "            answer_ids,\n",
        "            sep_ids,\n",
        "            final_answer_ids\n",
        "        ], dim=0)\n",
        "\n",
        "        # Only answer_ids contribute to the loss\n",
        "        label_ids = torch.cat([\n",
        "            torch.ones_like(question_ids) * -100,\n",
        "            torch.ones_like(answer_ids) * -100,\n",
        "            torch.ones_like(sep_ids) * -100,\n",
        "            final_answer_ids.clone()\n",
        "        ], dim=0)\n",
        "\n",
        "        # Stop early if this Q,A pair is too long\n",
        "        if input_ids.shape[0] > self.max_tokens:\n",
        "            # Print reason for skipping\n",
        "            # print(f\"Skipping due to input_ids being too big. input_ids.shape[0] = {input_ids.shape[0]}.\")\n",
        "            return None\n",
        "\n",
        "        input_ids = input_ids.tolist()\n",
        "        label_ids = label_ids.tolist()\n",
        "\n",
        "        return {\n",
        "            'input_ids_list': input_ids,\n",
        "            'label_ids_list': label_ids\n",
        "        }\n",
        "\n",
        "    def clean_filter_sample_peeking_gpt_eval(self, sample):\n",
        "        \"\"\"\n",
        "        Does the actual tokenization. Should be parallelized because it can be a bit slow.\n",
        "        \"\"\"\n",
        "\n",
        "        if sample == None:\n",
        "            return None\n",
        "\n",
        "        question, answer = sample\n",
        "\n",
        "        if self.clean_numbers:\n",
        "            question = _clean_numbers(question)\n",
        "            answer = _clean_numbers(answer)\n",
        "\n",
        "        answer_final = last_boxed_only_string(answer)\n",
        "\n",
        "        question_ids = torch.LongTensor(\n",
        "            self.tokenizer.encode(\"\\nQUESTION:\\n\" + question + \"\\nFULL SOLUTION:\\n\", verbose=False))\n",
        "        answer_ids = self.tokenizer.tokenize(answer)\n",
        "        answer_ids_full = torch.LongTensor(self.tokenizer.encode(answer))\n",
        "        answer_ids = only_until_first_boxed_from_tokens(answer, answer_ids)\n",
        "        if len(answer_ids) == 0:\n",
        "            return None\n",
        "        answer_ids = torch.LongTensor(self.tokenizer.encode(answer_ids, verbose=False))\n",
        "\n",
        "        # Take a fraction\n",
        "        if isinstance(self.peek_fraction, tuple):\n",
        "            final_idx = int(len(answer_ids) * random.uniform(*self.peek_fraction))\n",
        "        else:\n",
        "            final_idx = int(len(answer_ids) * self.peek_fraction)\n",
        "\n",
        "        answer_ids = answer_ids[:final_idx]\n",
        "\n",
        "        # sep_ids          = torch.LongTensor(self.tokenizer.encode(\"\\nFINAL ANSWER\\n\", verbose=False))\n",
        "        final_answer_ids = answer_ids_full[final_idx:]\n",
        "        print(final_answer_ids)\n",
        "\n",
        "        input_ids = torch.cat([\n",
        "            question_ids,\n",
        "            answer_ids,\n",
        "            # sep_ids,\n",
        "        ], dim=0)\n",
        "\n",
        "        # Only answer_ids contribute to the loss\n",
        "        label_ids = torch.cat([\n",
        "            final_answer_ids.clone()\n",
        "        ], dim=0)\n",
        "\n",
        "        # Stop early if this Q,A pair is too long\n",
        "        if input_ids.shape[0] + label_ids.shape[0] > self.max_tokens:\n",
        "            # Print reason for skipping\n",
        "            # print(f\"Skipping due to input_ids being too big. input_ids.shape[0] = {input_ids.shape[0]}.\")\n",
        "            return None\n",
        "\n",
        "        input_ids = input_ids.tolist()\n",
        "        label_ids = label_ids.tolist()\n",
        "\n",
        "        return {\n",
        "            'input_ids_list': input_ids,\n",
        "            'label_ids_list': label_ids\n",
        "        }\n",
        "\n",
        "    def clean_filter_sample_gpt_eval(self, sample):\n",
        "        \"\"\"\n",
        "        Does tokenization for final model evaluation. This should return\n",
        "        input_ids as the context and labels as the true answer.\n",
        "        \"\"\"\n",
        "\n",
        "        if sample == None:\n",
        "            return None\n",
        "\n",
        "        if self.mode_answer == 'eval_peeking':\n",
        "            return self.clean_filter_sample_peeking_gpt_eval(sample)\n",
        "        elif self.mode_answer == 'eval_nopack_padding':\n",
        "            return self.clean_filter_sample_nopackpadding_gpt_eval(sample)\n",
        "\n",
        "        question, answer = sample\n",
        "        print(\"question_from_sample: \" + question)\n",
        "        print(\"answer from sample: \" + answer)\n",
        "\n",
        "        if self.clean_numbers:\n",
        "            question = _clean_numbers(question)\n",
        "            answer = _clean_numbers(answer)\n",
        "            print(\"question: \" + question)\n",
        "            print(\"answer: \" + answer)\n",
        "        # answer_final = last_boxed_only_string(answer)\n",
        "        # print(\"answer_final: \" + str(answer_final))\n",
        "\n",
        "        assert not answer.isspace()\n",
        "\n",
        "        question_ids = torch.LongTensor(self.tokenizer.encode(\"\\nQUESTION:\\n\" + question, verbose=False))\n",
        "        sep_ids = torch.LongTensor(self.tokenizer.encode(\"\\FULL SOLUTION:\\n\", verbose=False))\n",
        "        answer_final_ids = torch.LongTensor(\n",
        "            self.tokenizer.encode(answer, verbose=False))  # Loss only counted on these tokens.\n",
        "\n",
        "        input_ids = torch.cat([\n",
        "            question_ids,\n",
        "            sep_ids,\n",
        "        ], dim=0)\n",
        "\n",
        "        label_ids = torch.cat([\n",
        "            answer_final_ids.clone()\n",
        "        ], dim=0)\n",
        "\n",
        "        # Stop early if this Q,A pair is too long\n",
        "        if input_ids.shape[0] + label_ids.shape[0] > self.max_tokens:\n",
        "            # Print reason for skipping\n",
        "            # print(f\"Skipping due to input_ids being too big. input_ids.shape[0] = {input_ids.shape[0]}.\")\n",
        "            return None\n",
        "\n",
        "        return {\n",
        "            'input_ids_list': input_ids.tolist(),\n",
        "            'label_ids_list': label_ids.tolist()\n",
        "        }\n"
      ],
      "metadata": {
        "id": "Rqm95sg6xLUS"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "aTqUcVxZy07S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fine-tuning"
      ],
      "metadata": {
        "id": "K4etm7OjzEXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### import necessary modules"
      ],
      "metadata": {
        "id": "lr5hvI8OzKm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install os\n",
        "# !pip install pprint\n",
        "# !pip install argparse\n",
        "# !pip install transformers\n",
        "# !pip install torch\n",
        "\n",
        "import os\n",
        "import pprint\n",
        "import argparse\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "L5ID4ckZzQNw"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get a tokenizer"
      ],
      "metadata": {
        "id": "gl-7cTra1J7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokenizer_gpt(arch):\n",
        "    \"\"\"\n",
        "    If args.tokenizer_merges_file is given, return a tokenizer that uses that merges_file.\n",
        "    In the paper, we use this to restrict models to ingest and outuput digits. For example:\n",
        "\n",
        "    >>> tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\", merges_file=\"merges_gpt2_single_digit_numbers.txt\")\n",
        "    >>> tokenizer_old = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    >>> tokenizer.encode(\"1\")\n",
        "    [16]\n",
        "    >>> tokenizer_old.encode(\"1\")\n",
        "    [16]\n",
        "    >>> tokenizer.encode(\"2\")\n",
        "    [17]\n",
        "    >>> tokenizer_old.encode(\"12\")\n",
        "    [1065]\n",
        "    >>> tokenizer.encode(\"12\")\n",
        "    [16, 17]\n",
        "    >>> tokenizer.encode(\"HEllo world!\")\n",
        "    [13909, 18798, 995, 0]\n",
        "    >>> tokenizer_old.encode(\"HEllo world!\")\n",
        "    [13909, 18798, 995, 0]\n",
        "    \"\"\"\n",
        "\n",
        "    tokenizer = transformers.GPT2Tokenizer.from_pretrained(arch)\n",
        "    return tokenizer\n"
      ],
      "metadata": {
        "id": "JIRBsLLC1Prr"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the dataset"
      ],
      "metadata": {
        "id": "q2bANpc607oq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(mathematica_dataroot = None, arch = 'gpt2'):\n",
        "    tokenizer = get_tokenizer_gpt(arch)\n",
        "    print(tokenizer)\n",
        "    # print(tokenizer.tokenize(\"1231231234441234 blah dklkjl12490!!@ 2*x + y^k + f(x)\"))  # sanity check\n",
        "\n",
        "\n",
        "    train_data = []\n",
        "\n",
        "    if mathematica_dataroot:\n",
        "      for mathematica_dr in mathematica_dataroot:\n",
        "        print(mathematica_dr)\n",
        "        # Save path to txt file which contains all txt files of math problems and answers for a specific category\n",
        "        # Algebra\n",
        "        flist_find_roots = os.path.join(mathematica_dataroot, \"algebra/flist_find_roots.txt\")\n",
        "        flist_invert_function = os.path.join(mathematica_dataroot, \"algebra/flist_invert_function.txt\")\n",
        "\n",
        "        # Calculus\n",
        "        flist_derivatives = os.path.join(mathematica_dataroot, \"calculus/flist_derivatives.txt\")\n",
        "        flist_integrals = os.path.join(mathematica_dataroot, \"calculus/flist_integrals.txt\")\n",
        "\n",
        "        # Geometry\n",
        "        flist_polygons = os.path.join(mathematica_dataroot, \"geometry/flist_polygons.txt\")\n",
        "        flist_triangles = os.path.join(mathematica_dataroot, \"geometry/flist_triangles.txt\")\n",
        "\n",
        "        # Linear Algebra\n",
        "        flist_determinant = os.path.join(mathematica_dataroot, \"linear_algebra/flist_determinant.txt\")\n",
        "        flist_orthogonolize_vectors = os.path.join(mathematica_dataroot, \"linear_algebra/flist_orthogonolize_vectors.txt\")\n",
        "\n",
        "        with open(flist_find_roots, \"r\") as f:\n",
        "          find_roots_num_files = len(f.readlines())\n",
        "\n",
        "        with open(flist_invert_function, \"r\") as f:\n",
        "          invert_function_num_files = len(f.readlines())\n",
        "\n",
        "        with open(flist_derivatives, \"r\") as f:\n",
        "          derivatives_num_files = len(f.readlines())\n",
        "\n",
        "        with open(flist_integrals, \"r\") as f:\n",
        "          integrals_num_files = len(f.readlines())\n",
        "\n",
        "        with open(flist_polygons, \"r\") as f:\n",
        "          polygons_num_files = len(f.readlines())\n",
        "\n",
        "        with open(flist_triangles, \"r\") as f:\n",
        "          triangles_num_files = len(f.readlines())\n",
        "\n",
        "        with open(flist_determinant, \"r\") as f:\n",
        "          determinant_num_files = len(f.readlines())\n",
        "\n",
        "        with open(flist_orthogonolize_vectors, \"r\") as f:\n",
        "          orthogonolize_vectors_num_files = len(f.readlines())\n",
        "\n",
        "        if find_roots_num_files:\n",
        "          train_data.append(MathematicaMathDataset(\n",
        "            dataroot=flist_find_roots,\n",
        "            tokenizer=tokenizer,\n",
        "            max_tokens=384 if arch == 'gpt2-xl' else 1024,\n",
        "            mode='gpt2',\n",
        "          ))\n",
        "\n",
        "        if invert_function_num_files:\n",
        "          train_data.append(MathematicaMathDataset(\n",
        "            dataroot=flist_invert_function,\n",
        "            tokenizer=tokenizer,\n",
        "            max_tokens=384 if arch == 'gpt2-xl' else 1024,\n",
        "            mode='gpt2',\n",
        "          ))\n",
        "\n",
        "        if derivatives_num_files:\n",
        "          train_data.append(MathematicaMathDataset(\n",
        "            dataroot=flist_derivatives,\n",
        "            tokenizer=tokenizer,\n",
        "            max_tokens=384 if arch == 'gpt2-xl' else 1024,\n",
        "            mode='gpt2'\n",
        "          ))\n",
        "\n",
        "        if integrals_num_files:\n",
        "          train_data.append(MathematicaMathDataset(\n",
        "            dataroot=flist_integrals,\n",
        "            tokenizer=tokenizer,\n",
        "            max_tokens=384 if arch == 'gpt2-xl' else 1024,\n",
        "            mode='gpt2'\n",
        "          ))\n",
        "\n",
        "        if polygons_num_files:\n",
        "          train_data.append(MathematicaMathDataset(\n",
        "            dataroot=flist_polygons,\n",
        "            tokenizer=tokenizer,\n",
        "            max_tokens=384 if arch == 'gpt2-xl' else 1024,\n",
        "            mode='gpt2'\n",
        "          ))\n",
        "\n",
        "        if triangles_num_files:\n",
        "          train_data.append(MathematicaMathDataset(\n",
        "            dataroot=flist_triangles,\n",
        "            tokenizer=tokenizer,\n",
        "            max_tokens=384 if arch == 'gpt2-xl' else 1024,\n",
        "            mode='gpt2',\n",
        "          ))\n",
        "\n",
        "        if determinant_num_files:\n",
        "          train_data.append(MathematicaMathDataset(\n",
        "            dataroot=flist_determinant,\n",
        "            tokenizer=tokenizer,\n",
        "            max_tokens=384 if arch == 'gpt2-xl' else 1024,\n",
        "            mode='gpt2',\n",
        "          ))\n",
        "\n",
        "        if orthogonolize_vectors_num_files:\n",
        "          train_data.append(MathematicaMathDataset(\n",
        "            dataroot=flist_orthogonolize_vectors,\n",
        "            tokenizer=tokenizer,\n",
        "            max_tokens=384 if arch == 'gpt2-xl' else 1024,\n",
        "            mode='gpt2',\n",
        "          ))\n",
        "\n",
        "        # Print the sizes of each dataset, useful for weighting\n",
        "        for dset in train_data:\n",
        "          print(f\"{dset.__class__.__name__}: __len__ = {len(dset)}\")\n",
        "\n",
        "        return torch.utils.data.ConcatDataset(train_data)\n"
      ],
      "metadata": {
        "id": "ZeR_lpVh1B4n"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT2 Trainer"
      ],
      "metadata": {
        "id": "Jqee8vCGz4XO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Trainer(transformers.Trainer):\n",
        "    def create_optimizer_and_scheduler(self, num_training_steps: int):\n",
        "        \"\"\"\n",
        "        Setup the optimizer and the learning rate scheduler.\n",
        "        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n",
        "        Trainer's init through :obj:`optimizers`, or subclass and override this method in a subclass.\n",
        "        \"\"\"\n",
        "        if self.optimizer is None:\n",
        "            print(\"Making AdamW Optimizer\")\n",
        "            no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "            optimizer_grouped_parameters = [\n",
        "                {\n",
        "                    \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "                    \"weight_decay\": self.args.weight_decay,\n",
        "                },\n",
        "                {\n",
        "                    \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "                    \"weight_decay\": 0.0,\n",
        "                },\n",
        "            ]\n",
        "            self.optimizer = torch.optim.AdamW(\n",
        "                optimizer_grouped_parameters,\n",
        "                lr=self.args.learning_rate,\n",
        "                betas=(self.args.adam_beta1, self.args.adam_beta2),\n",
        "                eps=self.args.adam_epsilon,\n",
        "            )\n",
        "\n",
        "        if self.lr_scheduler is None:\n",
        "\n",
        "            if self.args.warmup_steps == -1:\n",
        "                print(\"Using constant LR\")\n",
        "                self.lr_scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lambda steps: 1.0)\n",
        "            else:\n",
        "                print(\"Using Linear warmup LR\")\n",
        "                self.lr_scheduler = self.get_linear_schedule_with_warmup(\n",
        "                    self.optimizer, num_warmup_steps=self.args.warmup_steps, num_training_steps=num_training_steps\n",
        "                )\n",
        "\n",
        "    @staticmethod\n",
        "    def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, last_epoch=-1):\n",
        "        \"\"\"\n",
        "        Linear warmup from 0 to max lr, then linear decay from max_lr to 0.1*max_lr\n",
        "        As done in https://arxiv.org/pdf/2010.14701.pdf\n",
        "        \"\"\"\n",
        "\n",
        "        def lr_lambda(current_step: int):\n",
        "            if current_step < num_warmup_steps:\n",
        "                return float(current_step) / float(max(1, num_warmup_steps))\n",
        "            min_lr_multiplier = 0.1\n",
        "            return max(\n",
        "                min_lr_multiplier,\n",
        "                ((1 - min_lr_multiplier) * float(num_training_steps - current_step) / float(\n",
        "                    max(1, num_training_steps - num_warmup_steps))) + min_lr_multiplier\n",
        "            )\n",
        "\n",
        "        return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch)"
      ],
      "metadata": {
        "id": "inmORgIEz6dx"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the training"
      ],
      "metadata": {
        "id": "hvKPMMEl0i0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_training(save_steps=0, tpu_num_cores=None, grad_acc_steps=4, batch_size_per_replica=8, load=None, arch='gpt2', save_dir='checkpoints/TEMP', epochs=2, lr=5e-5, weight_decay=0.05, lr_warmup_steps=1, log_freq=5, dataloader_num_workers=1, local_rank=-1, train_data=None):\n",
        "    save_dir = os.path.join(save_dir, datetime.now().strftime(\"%m-%d-%Y__%H:%M:%S\"))\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    if not save_steps:\n",
        "        # Save every epoch\n",
        "        if not tpu_num_cores:\n",
        "            save_steps = len(train_data)\n",
        "            # print(\"mps_is_available = \" + str(torch.backends.mps.is_available()))  # the MacOS is higher than 12.3+\n",
        "            # print(\"mps_is_built = \" + str(torch.backends.mps.is_built()))  # MPS is activated\n",
        "\n",
        "            # save_steps = int(save_steps / torch.cuda.device_count())\n",
        "            save_steps = int(save_steps / grad_acc_steps)\n",
        "            save_steps = int(save_steps / batch_size_per_replica)\n",
        "        else:\n",
        "            save_steps = len(train_data)\n",
        "            save_steps = int(save_steps / 8)  # 8 TPU cores is constant for now.\n",
        "            save_steps = int(save_steps / grad_acc_steps)\n",
        "            save_steps = int(save_steps / batch_size_per_replica)\n",
        "    else:\n",
        "        save_steps = save_steps\n",
        "\n",
        "    print(\"Save Steps = \", save_steps)\n",
        "\n",
        "    ## Checkpoint Loading ########################################################\n",
        "    if load:\n",
        "        model = transformers.GPT2LMHeadModel.from_pretrained(load)\n",
        "        print(f\"Loaded model from {load}\")\n",
        "    else:\n",
        "        model = transformers.GPT2LMHeadModel.from_pretrained(arch)\n",
        "\n",
        "    start_epoch = 0\n",
        "    start_iteration = 0\n",
        "\n",
        "    ## Dataloading ########################################################\n",
        "    train_data.start_iteration = start_iteration\n",
        "\n",
        "    ## Start Loop ########################################################\n",
        "    print(f\"Setting up Trainer\")\n",
        "\n",
        "    training_args = transformers.TrainingArguments(\n",
        "        output_dir=save_dir,\n",
        "        overwrite_output_dir=False,\n",
        "\n",
        "        do_train=True,\n",
        "        do_eval=False,\n",
        "        do_predict=True,\n",
        "        evaluation_strategy='no',\n",
        "        eval_steps=0,\n",
        "\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=batch_size_per_replica,\n",
        "        gradient_accumulation_steps=grad_acc_steps,\n",
        "\n",
        "        learning_rate=lr,\n",
        "        weight_decay=weight_decay,\n",
        "        warmup_steps=lr_warmup_steps,\n",
        "        max_grad_norm=100000.0,  # Essentially disable gradient clipping\n",
        "\n",
        "        logging_dir=save_dir,\n",
        "        logging_first_step=True,\n",
        "        logging_steps=log_freq,\n",
        "        save_steps=save_steps,\n",
        "        save_total_limit=10,  # Only save the last epoch\n",
        "\n",
        "        dataloader_drop_last=True,\n",
        "        dataloader_num_workers=dataloader_num_workers,\n",
        "\n",
        "        local_rank=local_rank,\n",
        "        tpu_num_cores=tpu_num_cores,\n",
        "    )\n",
        "\n",
        "    trainer = GPT2Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_data,\n",
        "    )\n",
        "    trainer.remove_callback(transformers.integrations.TensorBoardCallback)\n",
        "    # trainer.add_callback(CustomTensorBoardCallback())\n",
        "\n",
        "    print(f\"STARTING TRAINING. save_steps={save_steps}\")\n",
        "    trainer.train()\n",
        "\n",
        "    trainer.save_model(os.path.join(save_dir, \"final_checkpoint\"))\n",
        "    print(\"Finished\")\n"
      ],
      "metadata": {
        "id": "503iiFuO0mSa"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Specify additional args and start the training"
      ],
      "metadata": {
        "id": "xd3hcwuU1weq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    ######### Start training ###############################################################\n",
        "\n",
        "    train_data = get_dataset(mathematica_dataroot='/content/sample_data/train_data/')\n",
        "    run_training(train_data)\n",
        "\n",
        "\n",
        "def _mp_fn(index):\n",
        "    # For xla_spawn (TPUs)\n",
        "    main()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "l5pksXQb19E2",
        "outputId": "f1f7a83f-6131-4914-eb12-8646475d9595"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-133-8ba94caecf10>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-133-8ba94caecf10>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m######### Start training ###############################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmathematica_dataroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/sample_data/train_data/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-130-e0ce171272e3>\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(mathematica_dataroot, arch)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# Print the sizes of each dataset, useful for weighting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{dset.__class__.__name__}: __len__ = {len(dset)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConcatDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-125-2d7c59f33797>\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'MathematicaMathDataset' object has no attribute 'samples'"
          ]
        }
      ]
    }
  ]
}