{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJ3a4R8sMY8Yi9DviE8a7h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AngeloOttendorfer/Python_Projects/blob/master/Math_Language_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "rCHclPfcq7DR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {
        "id": "mId7wCNhtSE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def last_boxed_only(sample):\n",
        "    \"\"\"\n",
        "    Given a (q,a) sample, filter the answers so that they only contain\n",
        "    the last \\boxed{...} or \\fbox{...} element\n",
        "    \"\"\"\n",
        "    q, a = sample\n",
        "    a = last_boxed_only_string(a)\n",
        "    if a == None:\n",
        "        return None\n",
        "    return (q, a)\n",
        "\n",
        "def last_boxed_only_string(string):\n",
        "    idx = string.rfind(\"\\\\boxed\")\n",
        "    if idx < 0:\n",
        "        idx = string.rfind(\"\\\\fbox\")\n",
        "        if idx < 0:\n",
        "            return None\n",
        "\n",
        "    i = idx\n",
        "    right_brace_idx = None\n",
        "    num_left_braces_open = 0\n",
        "    while i < len(string):\n",
        "        if string[i] == \"{\":\n",
        "            num_left_braces_open += 1\n",
        "        if string[i] == \"}\":\n",
        "            num_left_braces_open -= 1\n",
        "            if num_left_braces_open == 0:\n",
        "                right_brace_idx = i\n",
        "                break\n",
        "        i += 1\n",
        "\n",
        "    if right_brace_idx == None:\n",
        "        retval = None\n",
        "    else:\n",
        "        retval = string[idx:right_brace_idx + 1]\n",
        "\n",
        "    return retval\n",
        "\n",
        "def only_until_first_boxed_from_tokens(string, tokens):\n",
        "    idx = string.find(\"\\\\boxed\")\n",
        "    if idx < 0:\n",
        "        idx = string.find(\"\\\\fbox\")\n",
        "        if idx < 0:\n",
        "            return None\n",
        "\n",
        "    cum_length = 0\n",
        "    for i, t in enumerate(tokens):\n",
        "        cum_length += len(t)\n",
        "        if cum_length >= idx:\n",
        "            break\n",
        "\n",
        "    return tokens[:i]\n",
        "\n",
        "def clean_numbers(sample):\n",
        "    if not sample:\n",
        "        return None\n",
        "    new_sample = list()\n",
        "    for s in sample:\n",
        "        new_sample.append(_clean_numbers(s))\n",
        "\n",
        "    return tuple(new_sample)\n",
        "\n",
        "def _clean_numbers(string):\n",
        "    \"\"\"\n",
        "    Clean Numbers in the given string\n",
        "\n",
        "    >>> _clean_numbers(None, \"Hello 123\")\n",
        "    'Hello 123'\n",
        "    >>> _clean_numbers(None, \"Hello 1234\")\n",
        "    'Hello 1,234'\n",
        "    >>> _clean_numbers(None, \"Hello 1234324asdasd\")\n",
        "    'Hello 1,234,324asdasd'\n",
        "    \"\"\"\n",
        "    num_prev_digits = 0\n",
        "    new_string = \"\"\n",
        "    for i, c in enumerate(string):\n",
        "        # isdigit() doesnt work here because of weird unicode chars.\n",
        "        if c in {'1', '2', '3', '4', '5', '6', '7', '8', '9', '0'}:\n",
        "            num_prev_digits += 1\n",
        "        else:\n",
        "            if num_prev_digits > 3:\n",
        "                # Some fixing\n",
        "                string_number = new_string[-num_prev_digits:]\n",
        "                new_string = new_string[:-num_prev_digits] + \"{0:,}\".format(int(string_number))\n",
        "            num_prev_digits = 0\n",
        "        new_string += c\n",
        "\n",
        "    if num_prev_digits > 3:\n",
        "        # Some fixing\n",
        "        string_number = new_string[-num_prev_digits:]\n",
        "        new_string = new_string[:-num_prev_digits] + \"{0:,}\".format(int(string_number))\n",
        "\n",
        "    return new_string"
      ],
      "metadata": {
        "id": "LTmYjikGtVRD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base Math Dataset"
      ],
      "metadata": {
        "id": "WqIDQyuPrBAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### import necessary modules"
      ],
      "metadata": {
        "id": "oSJ_Um81rMij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch\n",
        "# !pip install torch.nn.functional\n",
        "# !pip install random\n",
        "# !pip install os\n",
        "# !pip install time\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "FDw0-PpVrRQn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation"
      ],
      "metadata": {
        "id": "2yj-w5M_uUxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseMathDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Configurable AMPS Dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataroot, tokenizer, max_tokens, mode, mode_answer='default', len_multiplier=1.0, packing=None,\n",
        "                 randomize=None, pack_end=None, clean_numbers=True, latex_mask=False, peek_fraction=(0.1, 1.0)):\n",
        "        self.dataroot = dataroot\n",
        "        self.tokenizer = tokenizer  # Set in run_training(), not in dataset creation\n",
        "        self.max_tokens = max_tokens\n",
        "        self.mode = mode\n",
        "        self.mode_answer = mode_answer  # Used in subclass\n",
        "        self.len_multiplier = len_multiplier\n",
        "        self.clean_numbers = clean_numbers\n",
        "        self.latex_mask = latex_mask\n",
        "        self.peek_fraction = peek_fraction\n",
        "\n",
        "        if self.mode in {'gpt2'}:\n",
        "            self.clean_sample = self.clean_filter_sample_gpt\n",
        "            self.packing = True\n",
        "            self.randomize = True\n",
        "            self.include_fnames = False\n",
        "            self.pack_end = True\n",
        "        elif self.mode in {'gpt2-eval'}:\n",
        "            self.clean_sample = self.clean_filter_sample_gpt_eval\n",
        "            self.packing = True\n",
        "            self.randomize = False\n",
        "            self.include_fnames = True\n",
        "            self.pack_end = True\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "        if packing != None:\n",
        "            print(\"Overriding packing to be\", packing)\n",
        "            self.packing = packing\n",
        "        if randomize != None:\n",
        "            print(\"Overriding randomize to be\", randomize)\n",
        "            self.randomize = randomize\n",
        "        if pack_end != None:\n",
        "            print(\"Overriding pack_end to be\", pack_end)\n",
        "            self.pack_end = pack_end\n",
        "\n",
        "        self.initialize()\n",
        "\n",
        "        self.bad_fnames = set()\n",
        "        self.i = 0\n",
        "\n",
        "    def initialize(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def __len__(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # Each worker needs a different seed....\n",
        "        random.seed(os.getpid() + time.time() + random.random())\n",
        "\n",
        "        # Sampling with replacement.\n",
        "        # We need to pack random elements to get close to self.max_tokens\n",
        "        curr_input_ids = []\n",
        "        curr_label_ids = []\n",
        "        curr_fnames = []\n",
        "        num_samples = 0\n",
        "        while len(curr_input_ids) + 1 <= self.max_tokens and len(curr_label_ids) + 1 <= self.max_tokens:\n",
        "            # print(\"curr_input_ids: \" + str(curr_input_ids))\n",
        "            # print(\"curr_label_ids: \" + str(curr_label_ids))\n",
        "            # print(\"curr_fnames: \" + str(curr_fnames))\n",
        "            curr_sample, fname = self.get_random_sample()\n",
        "            # print(\"current_sample: \" + str(curr_sample))\n",
        "            # print(fname)\n",
        "            if curr_sample is None:\n",
        "                # This only happens in eval modes\n",
        "                return {\n",
        "                    \"input_ids\": torch.zeros([self.max_tokens]),\n",
        "                    \"labels\": torch.zeros([self.max_tokens]),\n",
        "                    \"fnames\": [fname]\n",
        "                }\n",
        "\n",
        "            if not self.pack_end and (\n",
        "                    (len(curr_input_ids) + 1 + len(curr_sample['input_ids_list']) > self.max_tokens) or\n",
        "                    (len(curr_label_ids) + 1 + len(curr_sample['label_ids_list']) > self.max_tokens)\n",
        "            ):\n",
        "                # Do not include curr_sample if either the input_ids or the label_ids will run off the end.\n",
        "                break\n",
        "\n",
        "            # Add curr_sample to the current inputs and labels\n",
        "            # print(\"input_ids_list: \" + str(curr_sample['input_ids_list']))\n",
        "            curr_input_ids.extend(curr_sample['input_ids_list'])\n",
        "            curr_label_ids.extend(curr_sample['label_ids_list'])\n",
        "            curr_fnames.append(fname)\n",
        "\n",
        "            num_samples += 1\n",
        "\n",
        "            # Break on the first iteration if we don't want to do packing.\n",
        "            if not self.packing:\n",
        "                break\n",
        "\n",
        "        input_ids = torch.LongTensor(curr_input_ids)\n",
        "        label_ids = torch.LongTensor(curr_label_ids)\n",
        "\n",
        "        # Sanity check\n",
        "        if 'eval' not in self.mode:\n",
        "            assert len(curr_input_ids) == len(curr_label_ids)\n",
        "\n",
        "        input_ids = input_ids[:self.max_tokens]\n",
        "        label_ids = label_ids[:self.max_tokens]\n",
        "\n",
        "        if len(curr_input_ids) < self.max_tokens and 'eval' not in self.mode:\n",
        "            # Pad\n",
        "            num_to_pad = self.max_tokens - len(curr_input_ids)\n",
        "            input_ids = F.pad(input_ids, [0, num_to_pad], mode='constant', value=self.tokenizer.pad_token_id)\n",
        "\n",
        "        if len(curr_label_ids) < self.max_tokens and 'eval' not in self.mode:\n",
        "            num_to_pad = self.max_tokens - len(curr_label_ids)\n",
        "            label_ids = F.pad(label_ids, [0, num_to_pad], mode='constant', value=-100)\n",
        "\n",
        "        # Sanity check\n",
        "        if 'eval' not in self.mode:\n",
        "            assert input_ids.shape[0] == label_ids.shape[\n",
        "                0] == self.max_tokens, f\"{input_ids.shape[0]}, {label_ids.shape[0]}, {self.max_tokens}\"\n",
        "\n",
        "        if self.include_fnames:\n",
        "            return {\n",
        "                \"input_ids\": input_ids,\n",
        "                \"labels\": label_ids,\n",
        "                \"fnames\": curr_fnames\n",
        "            }\n",
        "        else:\n",
        "            # This is the format required by our GPT2Trainer class\n",
        "            return {\n",
        "                \"input_ids\": input_ids,\n",
        "                \"labels\": label_ids\n",
        "            }\n",
        "\n",
        "    def get_random_sample(self):\n",
        "        \"\"\"\n",
        "        Get a full on random sample (used for training)\n",
        "        \"\"\"\n",
        "        random_sample = None\n",
        "        while random_sample is None:\n",
        "            if self.randomize:\n",
        "                q, a, fname = random.choice(self.samples)\n",
        "            else:\n",
        "                q, a, fname = self.samples[self.i]\n",
        "                self.i = (self.i + 1) % len(self.samples)\n",
        "\n",
        "            random_sample = self.clean_sample((q, a))  # q + '\\n' + a  # self.clean_sample((q, a))\n",
        "            # print(\"random_sample: \" + str(random_sample))\n",
        "\n",
        "            if not self.randomize:\n",
        "                break\n",
        "\n",
        "        return random_sample, fname\n",
        "\n",
        "    def clean_filter_sample_gpt(self, sample):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def clean_filter_sample_gpt_eval(self, sample):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def clean_filter_sample_t5(self, sample):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def clean_filter_sample_t5_eval(self, sample):\n",
        "        raise NotImplementedError()\n"
      ],
      "metadata": {
        "id": "OGv0I9SLq_CP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configurable Mathematica Dataset"
      ],
      "metadata": {
        "id": "FEcedn3BspKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import necessary modules"
      ],
      "metadata": {
        "id": "A5EIu8e3uhrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch\n",
        "# !pip install tqdm\n",
        "# !pip install os\n",
        "import torch\n",
        "import tqdm\n",
        "import os"
      ],
      "metadata": {
        "id": "_E6t2MQfupVA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Implementation"
      ],
      "metadata": {
        "id": "mGV7vynivyFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MathematicaMathDataset(BaseMathDataset):\n",
        "    \"\"\"Configurable Math Dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(len(self.samples) * self.len_multiplier)\n",
        "\n",
        "    def initialize(self):\n",
        "        \"\"\"\n",
        "        Set up self.samples by loading from the dataroot\n",
        "        \"\"\"\n",
        "\n",
        "        with open(self.dataroot, 'r') as fp:\n",
        "            all_filenames = fp.readlines()\n",
        "\n",
        "        print(f\"{self.__class__.__name__}: Loading samples from {len(all_filenames)} files.\")\n",
        "        samples_raw = []\n",
        "        for fname in tqdm(all_filenames):\n",
        "            fname = fname.rstrip()\n",
        "            # print(fname)\n",
        "            # fname = os.path.join(os.path.dirname(os.path.dirname(self.dataroot)), fname[2:])\n",
        "            # print(fname)\n",
        "\n",
        "            if not os.path.isfile(fname):\n",
        "                print(f\"SKIPPING {fname}\")\n",
        "                continue\n",
        "            with open(fname, 'r') as fp:\n",
        "                question = \"\"\n",
        "                answers  = []\n",
        "                reading_question = True\n",
        "                curr_section = \"\"\n",
        "                for line in fp:\n",
        "                    if line == \"Problem:\\n\":\n",
        "                        reading_question = True\n",
        "                    elif line == \"Answer:\\n\":\n",
        "                        if reading_question:\n",
        "                            # curr_section contains Q\n",
        "                            question = curr_section\n",
        "                        else:\n",
        "                            # curr_section contains an A\n",
        "                            answers.append(curr_section)\n",
        "                        curr_section = \"\"\n",
        "                        reading_question = False\n",
        "                    else:\n",
        "                        curr_section += line\n",
        "\n",
        "                # The last answer needs to be recorded.\n",
        "                answers.append(curr_section)\n",
        "\n",
        "            for a in answers:\n",
        "                samples_raw.append((question, a, fname))\n",
        "\n",
        "        # manager = Manager()\n",
        "        # samples_raw = manager.list(samples_raw)\n",
        "        self.samples = samples_raw\n",
        "        del samples_raw\n",
        "\n",
        "        print(f\"{self.__class__.__name__}: Loaded {len(self.samples)} samples.\")\n",
        "        # print(self.samples)\n",
        "\n",
        "    def clean_filter_sample_gpt(self, sample):\n",
        "        \"\"\"\n",
        "        Does the actual tokenization. Should be parallelized because it can be a bit slow.\n",
        "        \"\"\"\n",
        "\n",
        "        if sample == None:\n",
        "            return None\n",
        "\n",
        "        question, answer = sample\n",
        "        if self.clean_numbers:\n",
        "            question = _clean_numbers(question)\n",
        "            answer = _clean_numbers(answer)\n",
        "\n",
        "        if self.mode_answer == 'default':\n",
        "            question_ids     = torch.LongTensor(self.tokenizer.encode(\"\\nQUESTION:\\n\" + question, verbose=False))\n",
        "\n",
        "            sep_ids          = torch.LongTensor(self.tokenizer.encode(\"\\nFINAL ANSWER:\\n\", verbose=False))\n",
        "            answer_ids       = self.tokenizer.encode(answer, verbose=False)\n",
        "            answer_ids.append(self.tokenizer.eos_token_id)\n",
        "            answer_ids       = torch.LongTensor(answer_ids)\n",
        "\n",
        "            # Use full solution\n",
        "            input_ids = torch.cat([\n",
        "                question_ids,\n",
        "                sep_ids,\n",
        "                answer_ids\n",
        "            ], dim=0)\n",
        "\n",
        "            label_ids = torch.cat([\n",
        "                torch.ones_like(question_ids) * -100,\n",
        "                torch.ones_like(sep_ids) * -100,\n",
        "                answer_ids.clone()\n",
        "            ], dim=0)\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "        # Stop early if this Q,A pair is too long\n",
        "        if input_ids.shape[0] > self.max_tokens:\n",
        "            # Print reason for skipping\n",
        "            # print(f\"{self.__class__.__name__} Skipping due to input_ids being too big. input_ids.shape[0] = {input_ids.shape[0]}.\")\n",
        "            return None\n",
        "\n",
        "        input_ids = input_ids.tolist()\n",
        "        label_ids = label_ids.tolist()\n",
        "\n",
        "        return {\n",
        "            'input_ids_list' : input_ids,\n",
        "            'label_ids_list' : label_ids\n",
        "        }\n",
        "\n",
        "    def clean_filter_sample_t5(self, sample):\n",
        "        \"\"\"\n",
        "        Does the actual tokenization. Should be parallelized because it can be a bit slow.\n",
        "        \"\"\"\n",
        "\n",
        "        if sample == None:\n",
        "            return None\n",
        "\n",
        "        question, answer = sample\n",
        "        if self.clean_numbers:\n",
        "            question = _clean_numbers(question)\n",
        "            answer = _clean_numbers(answer)\n",
        "\n",
        "        if self.mode_answer == 'default':\n",
        "            question_ids     = torch.LongTensor(self.tokenizer.encode(\"\\nQUESTION:\\n\" + question + \"\\nFINAL ANSWER:\\n\", verbose=False))\n",
        "            answer_ids       = torch.LongTensor(self.tokenizer.encode(answer, verbose=False))\n",
        "\n",
        "            input_ids = torch.cat([\n",
        "                question_ids,\n",
        "            ], dim=0)\n",
        "\n",
        "            label_ids = torch.cat([\n",
        "                answer_ids\n",
        "            ], dim=0)\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "        # Stop early if this Q,A pair is too long\n",
        "        if input_ids.shape[0] > self.max_tokens:\n",
        "            # Print reason for skipping\n",
        "            # print(f\"{self.__class__.__name__} Skipping due to input_ids being too big. input_ids.shape[0] = {input_ids.shape[0]}.\")\n",
        "            return None\n",
        "\n",
        "        input_ids = input_ids.tolist()\n",
        "        label_ids = label_ids.tolist()\n",
        "\n",
        "        return {\n",
        "            'input_ids_list' : input_ids,\n",
        "            'label_ids_list' : label_ids\n",
        "        }\n"
      ],
      "metadata": {
        "id": "WVr1T27xtDPJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BCtvq5gSsnES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MATH Dataset Configuration"
      ],
      "metadata": {
        "id": "XzO0KdTvwMSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### import necessary modules"
      ],
      "metadata": {
        "id": "_nhBVNWhwTMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch\n",
        "# !pip install json\n",
        "# !pip install glob\n",
        "# !pip install random\n",
        "# !pip install numpy\n",
        "\n",
        "import torch\n",
        "import json\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from multiprocessing import Manager"
      ],
      "metadata": {
        "id": "cndWlUKEwW5o"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation"
      ],
      "metadata": {
        "id": "3UqmXtDOxI_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MATHDataset(BaseMathDataset):\n",
        "    \"\"\"Configurable Math Dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(len(self.samples) * self.len_multiplier)\n",
        "\n",
        "    def initialize(self):\n",
        "        \"\"\"\n",
        "        Set up self.samples by loading from the dataroot\n",
        "        \"\"\"\n",
        "        print(self.dataroot)\n",
        "        all_filenames = glob.glob('/Users/angeloottendorfer/Desktop/amps/mathematica/algebra/testdaten_find_roots/*')\n",
        "        print(all_filenames)\n",
        "        samples_raw = []\n",
        "        for fname in all_filenames:\n",
        "            with open(fname, 'r') as fp:\n",
        "                try:\n",
        "                    problem_data = json.load(fp)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading JSON from {fname}\", e)\n",
        "                    raise e\n",
        "            curr_sample_raw = (problem_data['problem'], problem_data['solution'], fname)\n",
        "            for e in curr_sample_raw:\n",
        "                assert e\n",
        "            samples_raw.append(curr_sample_raw)\n",
        "\n",
        "        manager = Manager()\n",
        "        samples_raw = manager.list(samples_raw)\n",
        "        self.samples = samples_raw\n",
        "        del samples_raw\n",
        "\n",
        "        print(f\"{self.__class__.__name__}: Loaded {len(self.samples)} samples.\")\n",
        "\n",
        "    def clean_filter_sample_gpt(self, sample):\n",
        "        \"\"\"\n",
        "        Does the actual tokenization. Should be parallelized because it can be a bit slow.\n",
        "        \"\"\"\n",
        "\n",
        "        if sample == None:\n",
        "            return None\n",
        "\n",
        "        if self.mode_answer == 'peeking_only':\n",
        "            return self.clean_filter_sample_peeking_gpt(sample)\n",
        "        if self.mode_answer == 'mixed_full_and_peeking':\n",
        "            if random.random() < 0.5:\n",
        "                return self.clean_filter_sample_peeking_gpt(sample)\n",
        "            else:\n",
        "                _mode_answer = 'full'\n",
        "        elif self.mode_answer == 'mixed_full_and_nopack_padding':\n",
        "            if random.random() < 0.5:\n",
        "                return self.clean_filter_sample_nopackpadding_gpt(sample)\n",
        "            else:\n",
        "                _mode_answer = 'full'\n",
        "        elif self.mode_answer == 'mixed_final_boxed_and_full':\n",
        "            if random.random() < 0.5:\n",
        "                _mode_answer = 'full'\n",
        "            else:\n",
        "                _mode_answer = 'final_boxed'\n",
        "        elif self.mode_answer == 'full':\n",
        "            _mode_answer = 'full'\n",
        "        elif self.mode_answer == 'final_boxed':\n",
        "            _mode_answer = 'final_boxed'\n",
        "        else:\n",
        "            raise NotImplementedError(f\"self.mode_answer = {self.mode_answer} not recognized.\")\n",
        "\n",
        "        if _mode_answer == 'full':\n",
        "            question, answer = sample\n",
        "\n",
        "            if self.clean_numbers:\n",
        "                question = _clean_numbers(question)\n",
        "                answer = _clean_numbers(answer)\n",
        "\n",
        "            answer_final = last_boxed_only_string(answer)\n",
        "\n",
        "            question_ids = torch.LongTensor(self.tokenizer.encode(\"\\nQUESTION:\\n\" + question, verbose=False))\n",
        "\n",
        "            sep_ids_2 = torch.LongTensor(self.tokenizer.encode(\"\\nFULL SOLUTION:\\n\", verbose=False))\n",
        "            answer_ids = self.tokenizer.encode(answer, verbose=False)\n",
        "            answer_ids.append(self.tokenizer.eos_token_id)\n",
        "            answer_ids = torch.LongTensor(answer_ids)\n",
        "\n",
        "            input_ids = torch.cat([\n",
        "                question_ids,\n",
        "                sep_ids_2,\n",
        "                answer_ids\n",
        "            ], dim=0)\n",
        "\n",
        "            # Only answer_ids contribute to the loss\n",
        "            label_ids = torch.cat([\n",
        "                torch.ones_like(question_ids) * -100,\n",
        "                torch.ones_like(sep_ids_2) * -100,\n",
        "                answer_ids.clone()\n",
        "            ], dim=0)\n",
        "\n",
        "        elif _mode_answer == 'final_boxed':\n",
        "            question, answer = sample\n",
        "\n",
        "            if self.clean_numbers:\n",
        "                question = _clean_numbers(question)\n",
        "                answer = _clean_numbers(answer)\n",
        "            answer_final = last_boxed_only_string(answer)\n",
        "            if not answer_final:\n",
        "                print(\"ERROR FROM\", question, answer)\n",
        "                return None\n",
        "\n",
        "            question_ids = torch.LongTensor(self.tokenizer.encode(\"\\nQUESTION:\\n\" + question, verbose=False))\n",
        "\n",
        "            sep_ids_1 = torch.LongTensor(self.tokenizer.encode(\"\\nFINAL ANSWER:\\n\", verbose=False))\n",
        "            answer_final_ids = self.tokenizer.encode(answer_final, verbose=False)\n",
        "            answer_final_ids.append(self.tokenizer.eos_token_id)\n",
        "            answer_final_ids = torch.LongTensor(answer_final_ids)\n",
        "\n",
        "            input_ids = torch.cat([\n",
        "                question_ids,\n",
        "                sep_ids_1,\n",
        "                answer_final_ids,\n",
        "            ], dim=0)\n",
        "\n",
        "            # Only answer_ids contribute to the loss\n",
        "            label_ids = torch.cat([\n",
        "                torch.ones_like(question_ids) * -100,\n",
        "                torch.ones_like(sep_ids_1) * -100,\n",
        "                answer_final_ids.clone(),\n",
        "            ], dim=0)\n",
        "\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "        # Stop early if this Q,A pair is too long\n",
        "        if input_ids.shape[0] > self.max_tokens:\n",
        "            # Print reason for skipping\n",
        "            # print(f\"Skipping due to input_ids being too big. input_ids.shape[0] = {input_ids.shape[0]}.\")\n",
        "            return None\n",
        "\n",
        "        input_ids = input_ids.tolist()\n",
        "        label_ids = label_ids.tolist()\n",
        "\n",
        "        return {\n",
        "            'input_ids_list': input_ids,\n",
        "            'label_ids_list': label_ids\n",
        "        }\n",
        "\n",
        "    def clean_filter_sample_nopackpadding_gpt(self, sample):\n",
        "\n",
        "        if sample == None:\n",
        "            return None\n",
        "\n",
        "        question, answer = sample\n",
        "\n",
        "        if self.clean_numbers:\n",
        "            question = _clean_numbers(question)\n",
        "            answer = _clean_numbers(answer)\n",
        "\n",
        "        answer_final = last_boxed_only_string(answer)\n",
        "\n",
        "        question_ids = torch.LongTensor(self.tokenizer.encode(\"\\nQUESTION:\\n\" + question, verbose=False))\n",
        "        sep_ids = torch.LongTensor(self.tokenizer.encode(\"\\nFINAL ANSWER:\\n\", verbose=False))\n",
        "        final_answer_ids = torch.LongTensor(self.tokenizer.encode(answer_final, verbose=False))\n",
        "\n",
        "        # Stop early if this Q,A pair is too long\n",
        "        num_to_pad = 32\n",
        "        padding_tensor = torch.ones((num_to_pad)) * 220  # 220 is the token for space in the case of GPT2 models\n",
        "\n",
        "        input_ids = torch.cat([\n",
        "            question_ids,\n",
        "            padding_tensor,\n",
        "            sep_ids,\n",
        "            final_answer_ids\n",
        "        ], dim=0)\n",
        "\n",
        "        # Only answer_ids contribute to the loss\n",
        "        label_ids = torch.cat([\n",
        "            torch.ones_like(question_ids) * -100,\n",
        "            torch.ones_like(padding_tensor) * -100,\n",
        "            torch.ones_like(sep_ids) * -100,\n",
        "            final_answer_ids.clone()\n",
        "        ], dim=0)\n",
        "\n",
        "        input_ids = input_ids.tolist()\n",
        "        label_ids = label_ids.tolist()\n",
        "\n",
        "        return {\n",
        "            'input_ids_list': input_ids,\n",
        "            'label_ids_list': label_ids\n",
        "        }\n",
        "\n",
        "    def clean_filter_sample_nopackpadding_gpt_eval(self, sample):\n",
        "\n",
        "        if sample == None:\n",
        "            return None\n",
        "\n",
        "        question, answer = sample\n",
        "\n",
        "        if self.clean_numbers:\n",
        "            question = _clean_numbers(question)\n",
        "            answer = _clean_numbers(answer)\n",
        "\n",
        "        answer_final = last_boxed_only_string(answer)\n",
        "\n",
        "        question_ids = torch.LongTensor(self.tokenizer.encode(\"\\nQUESTION:\\n\" + question, verbose=False))\n",
        "        sep_ids = torch.LongTensor(self.tokenizer.encode(\"\\nFINAL ANSWER:\\n\", verbose=False))\n",
        "        final_answer_ids = torch.LongTensor(self.tokenizer.encode(answer_final, verbose=False))\n",
        "\n",
        "        num_to_pad = 32\n",
        "        padding_tensor = torch.ones((num_to_pad)) * 220  # 220 is the token for space in the case of GPT2 models\n",
        "\n",
        "        input_ids = torch.cat([\n",
        "            question_ids,\n",
        "            padding_tensor,\n",
        "            sep_ids,\n",
        "        ], dim=0)\n",
        "\n",
        "        # Only answer_ids contribute to the loss\n",
        "        label_ids = torch.cat([\n",
        "            final_answer_ids.clone()\n",
        "        ], dim=0)\n",
        "\n",
        "        # Stop early if this Q,A pair is too long\n",
        "        if input_ids.shape[0] + label_ids.shape[0] > self.max_tokens:\n",
        "            # Print reason for skipping\n",
        "            # print(f\"Skipping due to input_ids being too big. input_ids.shape[0] = {input_ids.shape[0]}.\")\n",
        "            return None\n",
        "\n",
        "        input_ids = input_ids.tolist()\n",
        "        label_ids = label_ids.tolist()\n",
        "\n",
        "        return {\n",
        "            'input_ids_list': input_ids,\n",
        "            'label_ids_list': label_ids\n",
        "        }\n",
        "\n",
        "    def clean_filter_sample_peeking_gpt(self, sample):\n",
        "        \"\"\"\n",
        "        Does the actual tokenization. Should be parallelized because it can be a bit slow.\n",
        "        \"\"\"\n",
        "\n",
        "        if sample == None:\n",
        "            return None\n",
        "\n",
        "        question, answer = sample\n",
        "\n",
        "        if self.clean_numbers:\n",
        "            question = _clean_numbers(question)\n",
        "            answer = _clean_numbers(answer)\n",
        "\n",
        "        answer_final = last_boxed_only_string(answer)\n",
        "\n",
        "        question_ids = torch.LongTensor(\n",
        "            self.tokenizer.encode(\"\\nQUESTION:\\n\" + question + \"\\nFULL SOLUTION:\\n\", verbose=False))\n",
        "        answer_ids = self.tokenizer.tokenize(answer)\n",
        "        answer_ids = only_until_first_boxed_from_tokens(answer, answer_ids)\n",
        "        answer_ids = torch.LongTensor(self.tokenizer.encode(answer_ids, verbose=False))\n",
        "\n",
        "        # Take a fraction\n",
        "        if isinstance(self.peek_fraction, tuple):\n",
        "            final_idx = int(len(answer_ids) * random.uniform(*self.peek_fraction))\n",
        "        else:\n",
        "            final_idx = int(len(answer_ids) * self.peek_fraction)\n",
        "\n",
        "        # # Override peeking fraction\n",
        "        # final_idx = int(len(answer_ids) * np.random.choice([0.25, 0.5, 0.75, 1.0], p=[1/6, 1/6, 1/3, 1/3]))\n",
        "\n",
        "        answer_ids = answer_ids[:final_idx]\n",
        "\n",
        "        sep_ids = torch.LongTensor(self.tokenizer.encode(\"\\nFINAL ANSWER:\\n\", verbose=False))\n",
        "        final_answer_ids = torch.LongTensor(self.tokenizer.encode(answer_ids[final_idx:]))\n",
        "\n",
        "        input_ids = torch.cat([\n",
        "            question_ids,\n",
        "            answer_ids,\n",
        "            sep_ids,\n",
        "            final_answer_ids\n",
        "        ], dim=0)\n",
        "\n",
        "        # Only answer_ids contribute to the loss\n",
        "        label_ids = torch.cat([\n",
        "            torch.ones_like(question_ids) * -100,\n",
        "            torch.ones_like(answer_ids) * -100,\n",
        "            torch.ones_like(sep_ids) * -100,\n",
        "            final_answer_ids.clone()\n",
        "        ], dim=0)\n",
        "\n",
        "        # Stop early if this Q,A pair is too long\n",
        "        if input_ids.shape[0] > self.max_tokens:\n",
        "            # Print reason for skipping\n",
        "            # print(f\"Skipping due to input_ids being too big. input_ids.shape[0] = {input_ids.shape[0]}.\")\n",
        "            return None\n",
        "\n",
        "        input_ids = input_ids.tolist()\n",
        "        label_ids = label_ids.tolist()\n",
        "\n",
        "        return {\n",
        "            'input_ids_list': input_ids,\n",
        "            'label_ids_list': label_ids\n",
        "        }\n",
        "\n",
        "    def clean_filter_sample_peeking_gpt_eval(self, sample):\n",
        "        \"\"\"\n",
        "        Does the actual tokenization. Should be parallelized because it can be a bit slow.\n",
        "        \"\"\"\n",
        "\n",
        "        if sample == None:\n",
        "            return None\n",
        "\n",
        "        question, answer = sample\n",
        "\n",
        "        if self.clean_numbers:\n",
        "            question = _clean_numbers(question)\n",
        "            answer = _clean_numbers(answer)\n",
        "\n",
        "        answer_final = last_boxed_only_string(answer)\n",
        "\n",
        "        question_ids = torch.LongTensor(\n",
        "            self.tokenizer.encode(\"\\nQUESTION:\\n\" + question + \"\\nFULL SOLUTION:\\n\", verbose=False))\n",
        "        answer_ids = self.tokenizer.tokenize(answer)\n",
        "        answer_ids_full = torch.LongTensor(self.tokenizer.encode(answer))\n",
        "        answer_ids = only_until_first_boxed_from_tokens(answer, answer_ids)\n",
        "        if len(answer_ids) == 0:\n",
        "            return None\n",
        "        answer_ids = torch.LongTensor(self.tokenizer.encode(answer_ids, verbose=False))\n",
        "\n",
        "        # Take a fraction\n",
        "        if isinstance(self.peek_fraction, tuple):\n",
        "            final_idx = int(len(answer_ids) * random.uniform(*self.peek_fraction))\n",
        "        else:\n",
        "            final_idx = int(len(answer_ids) * self.peek_fraction)\n",
        "\n",
        "        answer_ids = answer_ids[:final_idx]\n",
        "\n",
        "        # sep_ids          = torch.LongTensor(self.tokenizer.encode(\"\\nFINAL ANSWER\\n\", verbose=False))\n",
        "        final_answer_ids = answer_ids_full[final_idx:]\n",
        "        print(final_answer_ids)\n",
        "\n",
        "        input_ids = torch.cat([\n",
        "            question_ids,\n",
        "            answer_ids,\n",
        "            # sep_ids,\n",
        "        ], dim=0)\n",
        "\n",
        "        # Only answer_ids contribute to the loss\n",
        "        label_ids = torch.cat([\n",
        "            final_answer_ids.clone()\n",
        "        ], dim=0)\n",
        "\n",
        "        # Stop early if this Q,A pair is too long\n",
        "        if input_ids.shape[0] + label_ids.shape[0] > self.max_tokens:\n",
        "            # Print reason for skipping\n",
        "            # print(f\"Skipping due to input_ids being too big. input_ids.shape[0] = {input_ids.shape[0]}.\")\n",
        "            return None\n",
        "\n",
        "        input_ids = input_ids.tolist()\n",
        "        label_ids = label_ids.tolist()\n",
        "\n",
        "        return {\n",
        "            'input_ids_list': input_ids,\n",
        "            'label_ids_list': label_ids\n",
        "        }\n",
        "\n",
        "    def clean_filter_sample_gpt_eval(self, sample):\n",
        "        \"\"\"\n",
        "        Does tokenization for final model evaluation. This should return\n",
        "        input_ids as the context and labels as the true answer.\n",
        "        \"\"\"\n",
        "\n",
        "        if sample == None:\n",
        "            return None\n",
        "\n",
        "        if self.mode_answer == 'eval_peeking':\n",
        "            return self.clean_filter_sample_peeking_gpt_eval(sample)\n",
        "        elif self.mode_answer == 'eval_nopack_padding':\n",
        "            return self.clean_filter_sample_nopackpadding_gpt_eval(sample)\n",
        "\n",
        "        question, answer = sample\n",
        "        print(\"question_from_sample: \" + question)\n",
        "        print(\"answer from sample: \" + answer)\n",
        "\n",
        "        if self.clean_numbers:\n",
        "            question = _clean_numbers(question)\n",
        "            answer = _clean_numbers(answer)\n",
        "            print(\"question: \" + question)\n",
        "            print(\"answer: \" + answer)\n",
        "        # answer_final = last_boxed_only_string(answer)\n",
        "        # print(\"answer_final: \" + str(answer_final))\n",
        "\n",
        "        assert not answer.isspace()\n",
        "\n",
        "        question_ids = torch.LongTensor(self.tokenizer.encode(\"\\nQUESTION:\\n\" + question, verbose=False))\n",
        "        sep_ids = torch.LongTensor(self.tokenizer.encode(\"\\FULL SOLUTION:\\n\", verbose=False))\n",
        "        answer_final_ids = torch.LongTensor(\n",
        "            self.tokenizer.encode(answer, verbose=False))  # Loss only counted on these tokens.\n",
        "\n",
        "        input_ids = torch.cat([\n",
        "            question_ids,\n",
        "            sep_ids,\n",
        "        ], dim=0)\n",
        "\n",
        "        label_ids = torch.cat([\n",
        "            answer_final_ids.clone()\n",
        "        ], dim=0)\n",
        "\n",
        "        # Stop early if this Q,A pair is too long\n",
        "        if input_ids.shape[0] + label_ids.shape[0] > self.max_tokens:\n",
        "            # Print reason for skipping\n",
        "            # print(f\"Skipping due to input_ids being too big. input_ids.shape[0] = {input_ids.shape[0]}.\")\n",
        "            return None\n",
        "\n",
        "        return {\n",
        "            'input_ids_list': input_ids.tolist(),\n",
        "            'label_ids_list': label_ids.tolist()\n",
        "        }\n"
      ],
      "metadata": {
        "id": "Rqm95sg6xLUS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "aTqUcVxZy07S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fine-tuning"
      ],
      "metadata": {
        "id": "K4etm7OjzEXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### import necessary modules"
      ],
      "metadata": {
        "id": "lr5hvI8OzKm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install os\n",
        "# !pip install pprint\n",
        "# !pip install argparse\n",
        "# !pip install transformers\n",
        "# !pip install torch\n",
        "\n",
        "import os\n",
        "import pprint\n",
        "import argparse\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "L5ID4ckZzQNw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the dataset"
      ],
      "metadata": {
        "id": "q2bANpc607oq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(args):\n",
        "    tokenizer = get_tokenizer_gpt(args)\n",
        "    print(tokenizer)\n",
        "    # print(tokenizer.tokenize(\"1231231234441234 blah dklkjl12490!!@ 2*x + y^k + f(x)\"))  # sanity check\n",
        "\n",
        "    train_data = []\n",
        "\n",
        "    if args.mathematica_dataroot:\n",
        "        for mathematica_dr in args.mathematica_dataroot:\n",
        "            len_multiplier, dirname = mathematica_dr.split(\"@\")\n",
        "            len_multiplier = float(len_multiplier)\n",
        "            print(len_multiplier)\n",
        "\n",
        "            # Save path to txt file which contains all txt files of math problems and answers for a specific category\n",
        "            # Algebra\n",
        "            flist_find_roots = os.path.join(dirname, \"algebra/flist_find_roots.txt\")\n",
        "            flist_invert_function = os.path.join(dirname, \"algebra/flist_invert_function.txt\")\n",
        "\n",
        "            # Calculus\n",
        "            flist_derivatives = os.path.join(dirname, \"calculus/flist_derivatives.txt\")\n",
        "            flist_integrals = os.path.join(dirname, \"calculus/flist_integrals.txt\")\n",
        "\n",
        "            # Geometry\n",
        "            flist_polygons = os.path.join(dirname, \"geometry/flist_polygons.txt\")\n",
        "            flist_triangles = os.path.join(dirname, \"geometry/flist_triangles.txt\")\n",
        "\n",
        "            # Linear Algebra\n",
        "            flist_determinant = os.path.join(dirname, \"linear_algebra/flist_determinant.txt\")\n",
        "            flist_orthogonolize_vectors = os.path.join(dirname, \"linear_algebra/flist_orthogonolize_vectors.txt\")\n",
        "\n",
        "            with open(flist_find_roots, \"r\") as f:\n",
        "                find_roots_num_files = len(f.readlines())\n",
        "\n",
        "            with open(flist_invert_function, \"r\") as f:\n",
        "                invert_function_num_files = len(f.readlines())\n",
        "\n",
        "            with open(flist_derivatives, \"r\") as f:\n",
        "               derivatives_num_files = len(f.readlines())\n",
        "\n",
        "            with open(flist_integrals, \"r\") as f:\n",
        "                integrals_num_files = len(f.readlines())\n",
        "\n",
        "            with open(flist_polygons, \"r\") as f:\n",
        "                polygons_num_files = len(f.readlines())\n",
        "\n",
        "            with open(flist_triangles, \"r\") as f:\n",
        "                triangles_num_files = len(f.readlines())\n",
        "\n",
        "            with open(flist_determinant, \"r\") as f:\n",
        "                determinant_num_files = len(f.readlines())\n",
        "\n",
        "            with open(flist_orthogonolize_vectors, \"r\") as f:\n",
        "                orthogonolize_vectors_num_files = len(f.readlines())\n",
        "\n",
        "            if find_roots_num_files:\n",
        "                train_data.append(MathematicaMathDataset(\n",
        "                    dataroot=flist_find_roots,\n",
        "                    tokenizer=tokenizer,\n",
        "                    max_tokens=384 if args.arch == 'gpt2-xl' else 1024,\n",
        "                    mode='gpt2',\n",
        "                    len_multiplier=len_multiplier\n",
        "                ))\n",
        "\n",
        "            if invert_function_num_files:\n",
        "                train_data.append(MathematicaMathDataset(\n",
        "                    dataroot=flist_invert_function,\n",
        "                    tokenizer=tokenizer,\n",
        "                    max_tokens=384 if args.arch == 'gpt2-xl' else 1024,\n",
        "                    mode='gpt2',\n",
        "                    len_multiplier=len_multiplier\n",
        "                ))\n",
        "\n",
        "            if derivatives_num_files:\n",
        "                train_data.append(MathematicaMathDataset(\n",
        "                    dataroot=flist_derivatives,\n",
        "                    tokenizer=tokenizer,\n",
        "                    max_tokens=384 if args.arch == 'gpt2-xl' else 1024,\n",
        "                    mode='gpt2',\n",
        "                    len_multiplier=len_multiplier\n",
        "                ))\n",
        "\n",
        "            if integrals_num_files:\n",
        "                train_data.append(MathematicaMathDataset(\n",
        "                    dataroot=flist_integrals,\n",
        "                    tokenizer=tokenizer,\n",
        "                    max_tokens=384 if args.arch == 'gpt2-xl' else 1024,\n",
        "                    mode='gpt2',\n",
        "                    len_multiplier=len_multiplier\n",
        "                ))\n",
        "\n",
        "            if polygons_num_files:\n",
        "                train_data.append(MathematicaMathDataset(\n",
        "                    dataroot=flist_polygons,\n",
        "                    tokenizer=tokenizer,\n",
        "                    max_tokens=384 if args.arch == 'gpt2-xl' else 1024,\n",
        "                    mode='gpt2',\n",
        "                    len_multiplier=len_multiplier\n",
        "                ))\n",
        "\n",
        "            if triangles_num_files:\n",
        "                train_data.append(MathematicaMathDataset(\n",
        "                    dataroot=flist_triangles,\n",
        "                    tokenizer=tokenizer,\n",
        "                    max_tokens=384 if args.arch == 'gpt2-xl' else 1024,\n",
        "                    mode='gpt2',\n",
        "                    len_multiplier=len_multiplier\n",
        "                ))\n",
        "\n",
        "            if determinant_num_files:\n",
        "                train_data.append(MathematicaMathDataset(\n",
        "                    dataroot=flist_determinant,\n",
        "                    tokenizer=tokenizer,\n",
        "                    max_tokens=384 if args.arch == 'gpt2-xl' else 1024,\n",
        "                    mode='gpt2',\n",
        "                    len_multiplier=len_multiplier\n",
        "                ))\n",
        "\n",
        "            if orthogonolize_vectors_num_files:\n",
        "                train_data.append(MathematicaMathDataset(\n",
        "                    dataroot=flist_orthogonolize_vectors,\n",
        "                    tokenizer=tokenizer,\n",
        "                    max_tokens=384 if args.arch == 'gpt2-xl' else 1024,\n",
        "                    mode='gpt2',\n",
        "                    len_multiplier=len_multiplier\n",
        "                ))\n",
        "\n",
        "    # Print the sizes of each dataset, useful for weighting\n",
        "    for dset in train_data:\n",
        "        print(f\"{dset.__class__.__name__}: __len__ = {len(dset)}\")\n",
        "\n",
        "    return torch.utils.data.ConcatDataset(train_data)\n"
      ],
      "metadata": {
        "id": "ZeR_lpVh1B4n"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get a tokenizer"
      ],
      "metadata": {
        "id": "gl-7cTra1J7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokenizer_gpt(args):\n",
        "    \"\"\"\n",
        "    If args.tokenizer_merges_file is given, return a tokenizer that uses that merges_file.\n",
        "    In the paper, we use this to restrict models to ingest and outuput digits. For example:\n",
        "\n",
        "    >>> tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\", merges_file=\"merges_gpt2_single_digit_numbers.txt\")\n",
        "    >>> tokenizer_old = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    >>> tokenizer.encode(\"1\")\n",
        "    [16]\n",
        "    >>> tokenizer_old.encode(\"1\")\n",
        "    [16]\n",
        "    >>> tokenizer.encode(\"2\")\n",
        "    [17]\n",
        "    >>> tokenizer_old.encode(\"12\")\n",
        "    [1065]\n",
        "    >>> tokenizer.encode(\"12\")\n",
        "    [16, 17]\n",
        "    >>> tokenizer.encode(\"HEllo world!\")\n",
        "    [13909, 18798, 995, 0]\n",
        "    >>> tokenizer_old.encode(\"HEllo world!\")\n",
        "    [13909, 18798, 995, 0]\n",
        "    \"\"\"\n",
        "    if args.tokenizer_merges_file is not None:\n",
        "        tokenizer = transformers.GPT2Tokenizer.from_pretrained(args.arch, merges_file=args.tokenizer_merges_file)\n",
        "    else:\n",
        "        tokenizer = transformers.GPT2Tokenizer.from_pretrained(args.arch)\n",
        "    return tokenizer\n"
      ],
      "metadata": {
        "id": "JIRBsLLC1Prr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT2 Trainer"
      ],
      "metadata": {
        "id": "Jqee8vCGz4XO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Trainer(transformers.Trainer):\n",
        "    def create_optimizer_and_scheduler(self, num_training_steps: int):\n",
        "        \"\"\"\n",
        "        Setup the optimizer and the learning rate scheduler.\n",
        "        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n",
        "        Trainer's init through :obj:`optimizers`, or subclass and override this method in a subclass.\n",
        "        \"\"\"\n",
        "        if self.optimizer is None:\n",
        "            print(\"Making AdamW Optimizer\")\n",
        "            no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "            optimizer_grouped_parameters = [\n",
        "                {\n",
        "                    \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "                    \"weight_decay\": self.args.weight_decay,\n",
        "                },\n",
        "                {\n",
        "                    \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "                    \"weight_decay\": 0.0,\n",
        "                },\n",
        "            ]\n",
        "            self.optimizer = torch.optim.AdamW(\n",
        "                optimizer_grouped_parameters,\n",
        "                lr=self.args.learning_rate,\n",
        "                betas=(self.args.adam_beta1, self.args.adam_beta2),\n",
        "                eps=self.args.adam_epsilon,\n",
        "            )\n",
        "\n",
        "        if self.lr_scheduler is None:\n",
        "\n",
        "            if self.args.warmup_steps == -1:\n",
        "                print(\"Using constant LR\")\n",
        "                self.lr_scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lambda steps: 1.0)\n",
        "            else:\n",
        "                print(\"Using Linear warmup LR\")\n",
        "                self.lr_scheduler = self.get_linear_schedule_with_warmup(\n",
        "                    self.optimizer, num_warmup_steps=self.args.warmup_steps, num_training_steps=num_training_steps\n",
        "                )\n",
        "\n",
        "    @staticmethod\n",
        "    def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, last_epoch=-1):\n",
        "        \"\"\"\n",
        "        Linear warmup from 0 to max lr, then linear decay from max_lr to 0.1*max_lr\n",
        "        As done in https://arxiv.org/pdf/2010.14701.pdf\n",
        "        \"\"\"\n",
        "\n",
        "        def lr_lambda(current_step: int):\n",
        "            if current_step < num_warmup_steps:\n",
        "                return float(current_step) / float(max(1, num_warmup_steps))\n",
        "            min_lr_multiplier = 0.1\n",
        "            return max(\n",
        "                min_lr_multiplier,\n",
        "                ((1 - min_lr_multiplier) * float(num_training_steps - current_step) / float(\n",
        "                    max(1, num_training_steps - num_warmup_steps))) + min_lr_multiplier\n",
        "            )\n",
        "\n",
        "        return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch)"
      ],
      "metadata": {
        "id": "inmORgIEz6dx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the training"
      ],
      "metadata": {
        "id": "hvKPMMEl0i0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_training(args, train_data):\n",
        "    if not args.save_steps:\n",
        "        # Save every epoch\n",
        "        if not args.tpu_num_cores:\n",
        "            save_steps = len(train_data)\n",
        "            # print(\"mps_is_available = \" + str(torch.backends.mps.is_available()))  # the MacOS is higher than 12.3+\n",
        "            # print(\"mps_is_built = \" + str(torch.backends.mps.is_built()))  # MPS is activated\n",
        "\n",
        "            # save_steps = int(save_steps / torch.cuda.device_count())\n",
        "            save_steps = int(save_steps / args.grad_acc_steps)\n",
        "            save_steps = int(save_steps / args.batch_size_per_replica)\n",
        "        else:\n",
        "            save_steps = len(train_data)\n",
        "            save_steps = int(save_steps / 8)  # 8 TPU cores is constant for now.\n",
        "            save_steps = int(save_steps / args.grad_acc_steps)\n",
        "            save_steps = int(save_steps / args.batch_size_per_replica)\n",
        "    else:\n",
        "        save_steps = args.save_steps\n",
        "\n",
        "    print(\"Save Steps = \", save_steps)\n",
        "\n",
        "    ## Checkpoint Loading ########################################################\n",
        "    if args.load:\n",
        "        model = transformers.GPT2LMHeadModel.from_pretrained(args.load)\n",
        "        print(f\"Loaded model from {args.load}\")\n",
        "    else:\n",
        "        model = transformers.GPT2LMHeadModel.from_pretrained(args.arch)\n",
        "\n",
        "    start_epoch = 0\n",
        "    start_iteration = 0\n",
        "\n",
        "    ## Dataloading ########################################################\n",
        "    train_data.start_iteration = start_iteration\n",
        "\n",
        "    ## Start Loop ########################################################\n",
        "    print(f\"Setting up Trainer\")\n",
        "\n",
        "    training_args = transformers.TrainingArguments(\n",
        "        output_dir=args.save_dir,\n",
        "        overwrite_output_dir=False,\n",
        "\n",
        "        do_train=True,\n",
        "        do_eval=False,\n",
        "        do_predict=True,\n",
        "        evaluation_strategy='no',\n",
        "        eval_steps=0,\n",
        "\n",
        "        num_train_epochs=args.epochs,\n",
        "        per_device_train_batch_size=args.batch_size_per_replica,\n",
        "        gradient_accumulation_steps=args.grad_acc_steps,\n",
        "\n",
        "        learning_rate=args.lr,\n",
        "        weight_decay=args.weight_decay,\n",
        "        warmup_steps=args.lr_warmup_steps,\n",
        "        max_grad_norm=100000.0,  # Essentially disable gradient clipping\n",
        "\n",
        "        logging_dir=args.save_dir,\n",
        "        logging_first_step=True,\n",
        "        logging_steps=args.log_freq,\n",
        "        save_steps=save_steps,\n",
        "        save_total_limit=10,  # Only save the last epoch\n",
        "\n",
        "        dataloader_drop_last=True,\n",
        "        dataloader_num_workers=args.dataloader_num_workers,\n",
        "\n",
        "        local_rank=args.local_rank,\n",
        "        tpu_num_cores=args.tpu_num_cores,\n",
        "    )\n",
        "\n",
        "    trainer = GPT2Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_data,\n",
        "    )\n",
        "    trainer.remove_callback(transformers.integrations.TensorBoardCallback)\n",
        "    # trainer.add_callback(CustomTensorBoardCallback())\n",
        "\n",
        "    print(f\"STARTING TRAINING. save_steps={save_steps}\")\n",
        "    trainer.train()\n",
        "\n",
        "    trainer.save_model(os.path.join(args.save_dir, \"final_checkpoint\"))\n",
        "    print(\"Finished\")\n"
      ],
      "metadata": {
        "id": "503iiFuO0mSa"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Specify additional args and start the training"
      ],
      "metadata": {
        "id": "xd3hcwuU1weq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    ######### Arg parsing ###############################################################\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Language Modelling on Code\")\n",
        "    parser.add_argument('--arch', default='gpt2', choices=transformers.GPT2_PRETRAINED_MODEL_ARCHIVE_LIST)\n",
        "    parser.add_argument('--tokenizer-merges-file', default=None, type=str)\n",
        "    parser.add_argument('--load', default=None, type=str)\n",
        "\n",
        "    # Dataloading\n",
        "    parser.add_argument('--khan-mode', default='mixed_hints', type=str)\n",
        "    parser.add_argument('--khan-dataroot', default=None, type=str)\n",
        "    parser.add_argument('--khan-latex-mask', default=False, action='store_true')\n",
        "    parser.add_argument('--deepmind-dataroot', default=None, type=str, action='append')\n",
        "    parser.add_argument('--mathematica-dataroot', default='1@/content/sample_data/train_data/', type=str, action='append')\n",
        "    parser.add_argument('--mathematica-with-steps-dataroot', default=None, type=str, action='append')\n",
        "    parser.add_argument('--MATH-mode', default='mixed_final_boxed_and_full', type=str,\n",
        "                        choices=['mixed_final_boxed_and_full', 'final_boxed', 'peeking', 'nopack_padding',\n",
        "                                 'mixed_full_and_peeking', 'mixed_full_and_nopack_padding'])\n",
        "    parser.add_argument('--MATH-peek-min', default=0.1, type=float)\n",
        "    parser.add_argument('--MATH-peek-max', default=1.0, type=float)\n",
        "    parser.add_argument('--MATH-dataroot', default=None, type=str)\n",
        "    parser.add_argument('--stackexchange-dataroot', default=None, type=str)\n",
        "    parser.add_argument('--dataloader-num-workers', default=1, type=int)\n",
        "\n",
        "    # Training\n",
        "    parser.add_argument('--epochs', default=1, type=int)\n",
        "    parser.add_argument('--lr', default=5e-5, type=float)\n",
        "    parser.add_argument('--weight-decay', default=0.05, type=float)\n",
        "    parser.add_argument('--lr-warmup-steps', default=1, type=int)\n",
        "    parser.add_argument('--batch-size-per-replica', default=8, type=int)\n",
        "    parser.add_argument('--grad-acc-steps', default=4, type=int)\n",
        "    parser.add_argument('--local_rank', default=-1, type=int)\n",
        "    parser.add_argument('--tpu_num_cores', default=None, type=int)\n",
        "\n",
        "    # Logging and stuff\n",
        "    parser.add_argument('--save-dir', default=\"checkpoints/TEMP\", type=str)\n",
        "    parser.add_argument('--save-steps', default=0, type=int)\n",
        "    parser.add_argument('--log-freq', default=5, type=int)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    args.save_dir = os.path.join(args.save_dir, datetime.now().strftime(\"%m-%d-%Y__%H:%M:%S\"))\n",
        "\n",
        "    ######### Start training ###############################################################\n",
        "\n",
        "    argsdict = vars(args)\n",
        "    print(pprint.pformat(argsdict))\n",
        "\n",
        "    train_data = get_dataset(args)\n",
        "\n",
        "    os.makedirs(args.save_dir, exist_ok=True)\n",
        "    with open(os.path.join(args.save_dir, \"command.txt\"), 'w') as f:\n",
        "        f.write(pprint.pformat(argsdict))\n",
        "\n",
        "    run_training(args, train_data)\n",
        "\n",
        "\n",
        "def _mp_fn(index):\n",
        "    # For xla_spawn (TPUs)\n",
        "    main()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "l5pksXQb19E2",
        "outputId": "9dbd8724-9ae7-4a2e-fd19-d470a0fd35bf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--arch {gpt2,gpt2-medium,gpt2-large,gpt2-xl,distilgpt2}]\n",
            "                                [--tokenizer-merges-file TOKENIZER_MERGES_FILE] [--load LOAD]\n",
            "                                [--khan-mode KHAN_MODE] [--khan-dataroot KHAN_DATAROOT]\n",
            "                                [--khan-latex-mask] [--deepmind-dataroot DEEPMIND_DATAROOT]\n",
            "                                [--mathematica-dataroot MATHEMATICA_DATAROOT]\n",
            "                                [--mathematica-with-steps-dataroot MATHEMATICA_WITH_STEPS_DATAROOT]\n",
            "                                [--MATH-mode {mixed_final_boxed_and_full,final_boxed,peeking,nopack_padding,mixed_full_and_peeking,mixed_full_and_nopack_padding}]\n",
            "                                [--MATH-peek-min MATH_PEEK_MIN] [--MATH-peek-max MATH_PEEK_MAX]\n",
            "                                [--MATH-dataroot MATH_DATAROOT]\n",
            "                                [--stackexchange-dataroot STACKEXCHANGE_DATAROOT]\n",
            "                                [--dataloader-num-workers DATALOADER_NUM_WORKERS]\n",
            "                                [--epochs EPOCHS] [--lr LR] [--weight-decay WEIGHT_DECAY]\n",
            "                                [--lr-warmup-steps LR_WARMUP_STEPS]\n",
            "                                [--batch-size-per-replica BATCH_SIZE_PER_REPLICA]\n",
            "                                [--grad-acc-steps GRAD_ACC_STEPS] [--local_rank LOCAL_RANK]\n",
            "                                [--tpu_num_cores TPU_NUM_CORES] [--save-dir SAVE_DIR]\n",
            "                                [--save-steps SAVE_STEPS] [--log-freq LOG_FREQ]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-96cd7482-a6ce-473c-b108-0a3b5caea5bd.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ]
    }
  ]
}